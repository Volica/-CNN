{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef09e3b0",
   "metadata": {},
   "source": [
    "## 1、数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7691abb7",
   "metadata": {},
   "source": [
    "### 训练集数据收集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d436586",
   "metadata": {},
   "source": [
    "这段代码是用Python读取一个文本文件`dataset/train.txt`，该文件的内容似乎是将图像+文本路径与其对应的标签（情感类别）以逗号分隔的形式存储。每一行代表一个样本，格式为“图像+文本路径, 标签”。\n",
    "\n",
    "整个过程结束后，`path2label` 字典将包含训练集中每个图像路径及其对应的二元情感分类标签（在这里是三类：负面、中立、正面）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49475e9a-5ac9-4844-bc54-546bfcd5247c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-27T07:40:10.901598Z",
     "iopub.status.busy": "2023-06-27T07:40:10.901016Z",
     "iopub.status.idle": "2023-06-27T07:40:10.909043Z",
     "shell.execute_reply": "2023-06-27T07:40:10.908265Z",
     "shell.execute_reply.started": "2023-06-27T07:40:10.901562Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "path2label={}\n",
    "with open('dataset/train.txt', 'r') as f:\n",
    "    for line in f:\n",
    "\n",
    "        #print(line.strip())\n",
    "        path,label=line.split(',')\n",
    "        if label==\"negative\\n\":\n",
    "            label=0\n",
    "        elif label==\"neutral\\n\":\n",
    "            label=1\n",
    "        else:\n",
    "            label=2\n",
    "        path2label[path]=label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd966e2f",
   "metadata": {},
   "source": [
    "### 对于测试集的收集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "861adca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path2label1={}\n",
    "with open('dataset/test_without_label.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        # print(line.strip())\n",
    "        path,label=line.split(',')\n",
    "        if label==\"null\\n\":\n",
    "            label=3\n",
    "        path2label1[path]=label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0197acd0",
   "metadata": {},
   "source": [
    "### 下面完成poth2label和path2label1的加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c69c38-7cf1-44c3-9c49-8ab723e8b65d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-27T07:40:10.910728Z",
     "iopub.status.busy": "2023-06-27T07:40:10.910383Z",
     "iopub.status.idle": "2023-06-27T07:40:10.937721Z",
     "shell.execute_reply": "2023-06-27T07:40:10.936670Z",
     "shell.execute_reply.started": "2023-06-27T07:40:10.910702Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "path2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756dc19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path2label1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c886c87d",
   "metadata": {},
   "source": [
    "### 把train.txt/test.txt文件夹下面的所有的路径的txt/image写入同一个文件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b61e267-5df6-4168-8793-466c9e32d316",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-27T07:40:10.939166Z",
     "iopub.status.busy": "2023-06-27T07:40:10.938780Z",
     "iopub.status.idle": "2023-06-27T07:40:10.944477Z",
     "shell.execute_reply": "2023-06-27T07:40:10.943767Z",
     "shell.execute_reply.started": "2023-06-27T07:40:10.939141Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path=\"./dataset/data\"\n",
    "txt_files = [f+\".txt\" for f in path2label.keys()]\n",
    "img_files=[f+\".jpg\" for f in path2label.keys()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1598e044",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path=\"./dataset/data\"\n",
    "txt_files1 = [f+\".txt\" for f in path2label1.keys()]\n",
    "img_files1=[f+\".jpg\" for f in path2label1.keys()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ec68a0-cb8a-43d7-90db-abf2578cb233",
   "metadata": {},
   "source": [
    "### 训练集/测试集图片+文本收集\n",
    "- contents存储 训练集对应顺序的  文本信息\n",
    "- images存储  训练集对应顺序的 图片名称\n",
    "\n",
    "- contents1存储 测试集对应顺序的  文本信息\n",
    "- images1存储  测试集对应顺序的 图片名称"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e09851",
   "metadata": {},
   "source": [
    "这段代码是Python脚本的一部分，其目的是读取一个目录下的多个文本文件（`txt_files`是一个包含文件名列表的变量），并收集对应的文本内容、图片路径以及标签。每个文本文件的名字假设与某个特定图片的名字相关联，通过去掉`.txt`扩展名并在后面添加`.jpg`来得到对应图片的路径。\n",
    "\n",
    "这段代码的主要功能是从一系列文本文件中提取文本内容、关联的图片路径和标签信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd80b903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功读取文件数量: 4000\n",
      "无法读取文件数量: 0\n",
      "无法读取的文件路径列表: []\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "# txt=open(path+\"all_data.txt\",'w')\n",
    "\n",
    "can = 0\n",
    "cannot = 0\n",
    "contents = []   # 存储对应的文本\n",
    "images = []     # 存储相应顺序的图片\n",
    "labels = []\n",
    "failed_files = []\n",
    "\n",
    "encodings_to_try = ['utf-8', 'gbk', 'utf-16','ANSI']  # 添加更多可能的编码\n",
    "\n",
    "for i in txt_files:\n",
    "    file_path = path + \"/\" + i\n",
    "    image_path = file_path[:-4] + \".jpg\"\n",
    "    \n",
    "    content = None\n",
    "    for encoding in encodings_to_try:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as f:\n",
    "                content = f.read()\n",
    "                break  # 如果成功读取，则跳出循环\n",
    "\n",
    "        except UnicodeDecodeError:\n",
    "            pass  # 继续尝试下一个编码\n",
    "\n",
    "    if content is not None:\n",
    "        contents.append(content)\n",
    "        images.append(image_path)\n",
    "        labels.append(path2label[i[:-4]])\n",
    "        can += 1\n",
    "    else:\n",
    "        print(f\"无法识别文件 {file_path} 的编码\")\n",
    "        cannot += 1\n",
    "        failed_files.append(file_path)\n",
    "\n",
    "print(\"成功读取文件数量:\", can)\n",
    "print(\"无法读取文件数量:\", cannot)\n",
    "print(\"无法读取的文件路径列表:\", failed_files)\n",
    "print(len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a9a1dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功读取文件数量: 511\n",
      "无法读取文件数量: 0\n",
      "无法读取的文件路径列表: []\n",
      "511\n"
     ]
    }
   ],
   "source": [
    "# txt=open(path+\"all_data.txt\",'w')\n",
    "\n",
    "can = 0\n",
    "cannot = 0\n",
    "contents1 = []   # 存储对应的文本\n",
    "images1 = []     # 存储相应顺序的图片\n",
    "labels1 = []\n",
    "failed_files1 = []\n",
    "\n",
    "encodings_to_try = ['utf-8', 'gbk', 'utf-16','ANSI']  # 添加更多可能的编码\n",
    "\n",
    "for i in txt_files1:\n",
    "    file_path1 = path + \"/\" + i\n",
    "    image_path1 = file_path1[:-4] + \".jpg\"\n",
    "    \n",
    "    content1 = None\n",
    "    for encoding in encodings_to_try:\n",
    "        try:\n",
    "            with open(file_path1, 'r', encoding=encoding) as f:\n",
    "                content1 = f.read()\n",
    "                break  # 如果成功读取，则跳出循环\n",
    "\n",
    "        except UnicodeDecodeError:\n",
    "            pass  # 继续尝试下一个编码\n",
    "\n",
    "    if content1 is not None:\n",
    "        contents1.append(content1)\n",
    "        images1.append(image_path1)\n",
    "        # labels1.append(list(int(path2label1.keys()))[i])\n",
    "        label_key = i[:-4]\n",
    "        labels1.append(label_key)  # 将当前文件名作为键添\n",
    "        can += 1\n",
    "    else:\n",
    "        print(f\"无法识别文件 {file_path1} 的编码\")\n",
    "        cannot += 1\n",
    "        failed_files1.append(file_path1)\n",
    "\n",
    "print(\"成功读取文件数量:\", can)\n",
    "print(\"无法读取文件数量:\", cannot)\n",
    "print(\"无法读取的文件路径列表:\", failed_files1)\n",
    "print(len(images1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554afb5e",
   "metadata": {},
   "source": [
    "### 加载训练集的标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26638216-9749-4df3-91ff-bfe94b4993cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-27T07:40:13.068402Z",
     "iopub.status.busy": "2023-06-27T07:40:13.068018Z",
     "iopub.status.idle": "2023-06-27T07:40:13.084071Z",
     "shell.execute_reply": "2023-06-27T07:40:13.083414Z",
     "shell.execute_reply.started": "2023-06-27T07:40:13.068375Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013e8d77",
   "metadata": {},
   "source": [
    "### 加载测试集的序号（因为测试集的序号没有意义）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55d0766",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0efd6d1-5df6-42d8-a604-2b47d3af19c7",
   "metadata": {},
   "source": [
    "### 写一个文本转为数字的字典，将contents进行编码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8fe176",
   "metadata": {},
   "source": [
    "all_data.txt存放训练数据集的文字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f253b4b8-fdf6-4d9e-86de-562b4e905b06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-27T07:40:13.085842Z",
     "iopub.status.busy": "2023-06-27T07:40:13.085080Z",
     "iopub.status.idle": "2023-06-27T07:40:13.107060Z",
     "shell.execute_reply": "2023-06-27T07:40:13.090819Z",
     "shell.execute_reply.started": "2023-06-27T07:40:13.085812Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_data_path=\"all_data.txt\"\n",
    "with open(all_data_path, 'w') as f:\n",
    "    for data in contents:\n",
    "        f.write(data) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb2031c",
   "metadata": {},
   "source": [
    "存放测试集文字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "007709ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_path1=\"all_data1.txt\"\n",
    "with open(all_data_path1, 'w') as f:\n",
    "    for data1 in contents1:\n",
    "        f.write(data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9600c7",
   "metadata": {},
   "source": [
    "### 加载训练集/测试集文本（其中一个的输出被我手动清除了，为了减少占位）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6c3f6d-a673-4609-bc7c-99a4ad3e6c24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-27T07:40:13.108148Z",
     "iopub.status.busy": "2023-06-27T07:40:13.107900Z",
     "iopub.status.idle": "2023-06-27T07:40:13.131913Z",
     "shell.execute_reply": "2023-06-27T07:40:13.131098Z",
     "shell.execute_reply.started": "2023-06-27T07:40:13.108125Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc9c690d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Energetic training today with our San Antonio New Dollars/New Partners trainees \\n',\n",
       " 'Let your voice be heard! 18+ #endsuicide #blithe #selfharm #thinspo #bonespo #edo #hurt #cut \\n',\n",
       " \"RT @Austin_Powers__: Shark Week would be so much better if the sharks had laser beams attached to their frickin' heads. \\n\",\n",
       " '#TheTruthCaster http://t.co/S8jvqpKq5h\\n',\n",
       " \"RT @jarpad: Hey #WBSDCC look what we're up to!!!! @JensenAckles @paulwesley @iansomerhalder \\n\",\n",
       " '@CogTn: How do we get fresh oil? Separate the flesh from the SEED!#crushed @bryancutshall \\n',\n",
       " \"RT @DailyMailCeleb: Little Mix's Jesy has been showing off her engagement ring ? http://t.co/awZnuVyjDc http://t.co/YPDUXrrhgA\\n\",\n",
       " '@bexmader recently I got a #wicked t-shirt! ? you like it? ? \\n',\n",
       " 'Absolute loml, my beautiful & kind hearted baby boy \\n',\n",
       " '#Grim band logo of the day: Sabbat \\n',\n",
       " 'New #drawing #illustration #art #epic @2DArtbot #pen on #paper #awesome #strong #woman #girl #creativity #happy :3 \\n',\n",
       " 'RT @MrCocoyam: So my homeboy just got his heart broken \\n',\n",
       " 'Outdoor rehearsing is always fun hot pole podium & wobbly poles. @rockandbikefest are you ready to be Thunderstruck? \\n',\n",
       " 'New review up on my blog of An Imperfect Princess by Catherine Blakeney! \\n',\n",
       " 'This is a great quote. Dragons can be beaten! http://t.co/LFNcLzaIqs http://t.co/W7aPkoeqaj\\n',\n",
       " 'RT @billoberstjr: With the great @SuzeLanier at #EgyptianTheatre in Hollywood. Her new film #Cut is getting buzz htt… \\n',\n",
       " 'RT @ALLCAPSMARVEL: overjoyed - bastille http://t.co/Sp6ci4Zdwh\\n',\n",
       " 'RT @metrotoronto: Shooting victim Lecent Ross remembered as vibrant, loving and happy at Rexdale community vigil htt… \\n',\n",
       " 'EUC! Banana Republic Ruffle V Neck Button Down Cardigan Gray Small Beautiful $70 \\n',\n",
       " '#winter #northumberland #February #bleak #frozen #landscape #view #cloud #sun #frost #snow #dramaticsky #sky #storm… \\n',\n",
       " 'RT @Nunastic: Worshipful rabbits at Longthorpe Tower #BAAPboro #BAA2015 \\n',\n",
       " 'New York Giants Adult XXL Distressed-Style Zippered hooded jacket. http://t.co/EWaW0RJROW http://t.co/rDwl8dQlEs\\n',\n",
       " 'RT @terrellcwoods: Early run to @GrandCentralMkt. Really dig this place. Hoping the #springarcade can be as vibrant soon #mydayinla \\n',\n",
       " \"I could easily move into one of these houseboats in Victoria's Fisherman's Wharf. So bright and cheerful! \\n\",\n",
       " '#Extremely #Proud #Delighted #ScottishBand #BreakTheButterfly #HardRockRising #HardRockEdinburgh votes still coming \\n',\n",
       " 'RT @Weather_West: Remarkable,persistent warmth continues across entire Western U.S.;snowpack almost universally dismal.#cawx #cadrought htt… \\n',\n",
       " 'RT @SelinaAlko: THE CASE FOR LOVING is a Junior Library Guild selection! #overjoyed @sean_qualls \\n',\n",
       " '-De vita Beatae Mariae Magdalenae http://t.co/qL5QkOLhcF\\n',\n",
       " 'RT @artintheage: Sunny days & a SNAP Mint Lemonade: 1 part SNAP to 3 parts sparkling lemonade. Garnish with mint. Cheers! \\n',\n",
       " '  #FanArmyFaceOff #Directioners http://t…\\n',\n",
       " 'RT @BrionnaLondon: I hope my roommate cool cause this me every morning getting ready for class ? \\n',\n",
       " \"Daughter is infuriated by her father's wrong lyrics to 'Frozen': \\n\",\n",
       " 'RT @DailyESSEX: #Essex: Housing campaigners fuming after protest placards are stolen from gardens http://t.co/coFV5e3GQR http://t.co/deqXu1…\\n',\n",
       " \"RT @BarnhillKohlkkq: Fast and Furious won't be the same without you #TeamPW #RIPPaulWalker \\n\",\n",
       " 'Rough night #black eye #beaten #fxmakeup \\n',\n",
       " 'WE STILL AT IT D/L THE #outcAst EP HERE \\n',\n",
       " 'You dont need too many friends to be happy, the real ones are worth cherishing. Good night ? \\n',\n",
       " '#Broadway #Play \"#Disgraced\" #Pulitzer #Prize #Winner by #AmirKapoor. #NYC #TimesSquare #NewYork #Manhattan #Theater \\n',\n",
       " \"Great to meet @LucyBronze. Our hero! And we do not support B'ham! Sophie got flustered and messed her lines haha \\n\",\n",
       " \"RT @SimonNRicketts: Lewis Hamilton's empty seat at Wimbledon after he was turned away. (Story: \\n\",\n",
       " '@DayanNajar feeling so #inlove,#crush #infatuated, #wird #enjoyable #melting... And moré! \\n',\n",
       " '??CORAL ENHANCEMENT?? Want The SPARKLING 7/1 MAN UTD To Beat Preston (While It Lasts)? \\n',\n",
       " 'RT @leatherfaceword: @JenJenisha i am not gonna kill you... I am gonna hurt you really really bad ?? http://t.co/4wO6iRpdb5\\n',\n",
       " 'RT @nationaljournal: Supreme Court term limits are popular, bipartisan, and hopeless http://t.co/jk0yhzIHe7 http://t.co/KitivHcVTU\\n',\n",
       " 'New Deal #6701 HP 6530b Incomplete Laptop Notebook Intel 2.53GHz Core 2 Duo CPU 3GB RAM NO… \\n',\n",
       " \"RT @TheDSWF: Much maligned - Africa's most endangered carnivore. Find out more & donate to help them \\n\",\n",
       " 'RT @cuphaek: \"donghae why is the cookie jar empty?\" http://t.co/po7WkIknOg\\n',\n",
       " 'Bridges Burned (Entangled Teen) (G… \\n',\n",
       " 'RT @voodumamajuju: Barren waste land \\n',\n",
       " 'RT @fangirlwhowaits: @heatworld @Anythingbatch so please, before you say that all his fans are devastated and heartbroken... \\n',\n",
       " 'RT @GameOverGreggy: Just met Batman Beyond @willfriedle. Glad washed-up Batman Unlimited @RogerCraigSmith was there to ruin it. \\n',\n",
       " 'RT @kamrsa1: Just 4 days remaining take the duaa from your mother because your mother duaa can bless the happiness and more \\n',\n",
       " \"@GabbyLogan my son aged 12 born Down Syndrome and beaten Cancer twice took these photos he's amazing. \\n\",\n",
       " 'GM Ruston Webster talks Mariota signing: http://t.co/7jkuFc0HGA http://t.co/cbJ…\\n',\n",
       " '\" drink water whenever you think of food\" #empty #oops \\n',\n",
       " \"If you're curious of barker cypress #wrecked \\n\",\n",
       " '#FailedGOPLeaders 202-224-3121\\n',\n",
       " 'RT @ChaosChloe: #Chaotic Mind!! That was really heart felt. Thank u @SPARCEO \\n',\n",
       " 'Wedding photo bomb courtesy of random old man #stunned \\n',\n",
       " 'http://t.co/SiqBd7SSHN http://t.co/PbnuVINxs1\\n',\n",
       " \"RT @TheWeirdWorld: If you think depression isn't real... http://t.co/AmviyBx9Kp\\n\",\n",
       " 'RT @acya_5: 出来れば2位?とか言ってたのですが、どこのギルドさんも夜が活発で、そんな細かい調整無理ゲー全力投球でした??(?^ω^?)??まぁ、奇跡的に6人ラスアタミスという大チョンボして2位でした。計画通り(   ˙-˙   ) http://t.co/haDnb…\\n',\n",
       " 'Paralyzed?_? http://t.co/xLLOSfoKMF\\n',\n",
       " 'RT @Citi973: My encounter at Ghana’s ‘chaotic’ passport office- @UmaruSanda |See more here: #CitiNews \\n',\n",
       " '夜食のナン http://t.co/WVws6BSpdy\\n',\n",
       " '#AsILayDying #Forsaken #Music - We found \"As I Lay Dying\" - \"Forsaken\" for #free on audiom… \\n',\n",
       " \"RT @AnglinChanel: yu haven't lived til' you've chewed, sucked, & gnawed pon dis.. ????? #Caribbean #TeamJamaican \\n\",\n",
       " 'BEAUTY DAILY DEALS : http://t.co/JoJzMBzAMQ #8173 Nike The Eyes Lead The Body Eyeglass Storage Case \"Distressed\" … http://t.co/Ysr1JbC8Ul\\n',\n",
       " 'RT foxaustin \"State officials: Texas Gov. Abbott\\'s vetoes may be invalid http://t.co/Bawgq5xqQa #FOX7Austin http://t.co/8rJ3OF5JfT\"\\n',\n",
       " 'RT @AstroTerry: #speechless from this #sunrise \\n',\n",
       " 'Transpo Chair discusses connecting isolated comms w pub trans that imprv mobility but also tourism potential #NYmoves http://t.co/bnlK0PzXPN\\n',\n",
       " '#Time #approaches → 0 ← #and #thrilled #good !! \\n',\n",
       " 'Wild Photo: #Bleeding #Heart #Baboons \\n',\n",
       " '#WhyImSingle @justinbieber ruined my life. But in the best way possible ? \\n',\n",
       " \"Martha said for Valentine's Day she wanted a heart shaped pancake for lunch. Being the doting husband I am… \\n\",\n",
       " 'New of Steam on Ebay Grim Legends 2: Song of the Dark Swan Steam Key \\n',\n",
       " '? and I thought this was going to be a good day #hurt #JohnnyCash #sadturnup \\n',\n",
       " 'RT @kankore_erogazo: C85新刊、 http://t.co/2aOn3luE6t http://t.co/0VNHDSqLY7\\n',\n",
       " 'Marca | Casemiro will cut his holidays short and join Real Madrid sooner than expected. #HalaMadrid \\n',\n",
       " 'New kicks from @tenniswarehouse #delighted \\n',\n",
       " 'RT @ArianaGrande: pouty but overjoyed I swear ?? \\n',\n",
       " 'Wet n Wild Mega Last Matte Lipstick DollHouse Pink 967 Lip Color Ships Fast! \\n',\n",
       " \"RT @ZozeeBo: #SUGGMONDAY instead of #SUGGSUNDAY this week. Been in bed all day with lady-womb-pain so video isn't finished. DIS ME \\n\",\n",
       " 'RT @Mummydee23: Worship my feet you #pathetic #worthless #footboys get and lick the dirt and sweat off my #dirty #Soles #footfetish \\n',\n",
       " 'I just rescued Karma in #BestFiends - Download FREE - via @bestfiends \\n',\n",
       " \"@Melissa_Landers look what came in the mail today. I'm so excited to get started. Yay. #alienated \\n\",\n",
       " 'My stoner Stepdad installed a new porch light for me. Notice anything wrong? \\n',\n",
       " \"RT @Apostleshipsea: We're delighted to be part of the #BigWelfareDebate at #LISW15 http://t.co/eg6eSJaN7E @LISW15 #seafarers http://t.co/3h…\\n\",\n",
       " \"#fishing #Willow #Strike #Spinnerbait #Colorado #Bleeding Love the 'bleeding' red idea and… \\n\",\n",
       " '#Horrified #BMovie #Victims #Playset #Swagshop \\n',\n",
       " '#RT @vjalabert 5 Ways to Embed Your Organization With a #Culture of #Happiness via TLNT_com \\n',\n",
       " 'RT @Shyannaxx: When you realize you spelled something wrong in your tweet a day after it was posted @TaniseMarkia143 \\n',\n",
       " 'RT @YouChoices: Would you rather? \\n',\n",
       " 'Why are you feeling dispirited? Take the quiz: \\n',\n",
       " \"RT @MyDreamBiggest: @AndreaNavedo Jane's baby not even born and Xo is already drooling he ... It's going to be a doting grandmother. ?? htt… \\n\",\n",
       " 'RT @sillyharry: i lost my sneakers in Israel so now i have to use my grandmas :) \\n',\n",
       " \"I'm playing @pixtaword and I'm stumped with this one, can you help? \\n\",\n",
       " 'Birthday fruit cake! And on her left .... \\n',\n",
       " \"Dazed, confused, perplexed or just a sexy pose? I don't know which? \\n\",\n",
       " 'Todays premiere is sold out! Pulitzer Prize winner #Disgraced by Ayad Akhtar! \\n',\n",
       " 'I once got mistaken for Josh Todd of Buck Cherry ? \\n',\n",
       " 'Kung maging tayo, sayo lang ang puso ko. ?????? ???????? ?????? #Black&white #Happiness #eyebags \\n',\n",
       " 'RT @bdnews24: \"Congratulations Bangladesh Tigers. They have beaten Pak, Ind and now SA. This is the new Bangladesh.\": Shibly - ?AFP \\n',\n",
       " 'Dont be bitter today be better ??? \\n',\n",
       " \"RT @CNN: 'Minions' grabs second-biggest U.S. animated opening ever \\n\",\n",
       " 'Nada de novo. Tudo já foi visto muito por aí. Mas ainda t?o atual. Adoro?? #valentino #shoes #destroyed #jeans #de… \\n',\n",
       " 'Devoted to your sweetie? Show your commitment with a New Mod or Manufactured Home! #ShareTheLove @GoVillageHomes.com \\n',\n",
       " 'ef-a tale of melodise『ebullient future（Japanese）』  e6\" http://t.co/FnuwiZ9H5q\\n',\n",
       " 'Co-Parenting Guidelines for Estranged Couple | Iyanla: Fix My Life | ... - #howto #how \\n',\n",
       " '#SPEECHLESS. Deah & Yusor were just married in December. #ChapelHillShooting \\n',\n",
       " 'For any other Imperfect Parents - want to forgive yourself and join my club? \\n',\n",
       " 'RT @tanutinn: Japan PM mocked on Internet over model explanation http://t.co/xudJ7DM4yr http://t.co/H1Pi371U9E\\n',\n",
       " 'RT @pokellosexxy: Life has taught me that a person with no self Respect is incapable of respecting Others! \\n',\n",
       " \"RT @laurentbaheux: Cheetah's Eyes, Namibia 2004 ? print 40x40cm from 180\\ue76c - contact@laurentbaheux.com - #Wild http:… \\n\",\n",
       " \"Valentine's isn't really about the fancy surprises, it's about cherishing the whole moment with your loved one. \\n\",\n",
       " 'RT @HorrifyingPage: Found in an abandoned building. \\n',\n",
       " 'RT @haaretzcom: American Jews frustrated, alienated by Israel’s religious government http://t.co/yLBMsrAjyd http://t.co/Ww66FQMs46\\n',\n",
       " 'RT @BadgerBalmUK: Beat #midweek blues & #win Badger Cheerful Mind Balm. RT & follow @BadgerBalmsUK to enter. UK only Closes 12/07/2015. htt… \\n',\n",
       " 'RT @fIowerhun: i love how exo can be a double million seller but cant cut cake http://t.co/mF3w3Z5oni\\n',\n",
       " 'RT @DarthVadersBack: Harry Potter dressed as Boba Fett, your argument is invalid http://t.co/Xee5LJPC1q\\n',\n",
       " 'With temperatures around 90 and the humidity the result is the heat index below. Take it slow with the intense heat. \\n',\n",
       " '@CampInvention They are having SO MUCH FUN!  I love how enthusiastic they are when I pick them up each day! http://t.co/jGeWRQFAy8\\n',\n",
       " 'RT @Vanessa62114: Hey @Chobani where are all the toppings?? I was so looking forward to having my favorite yogurt! #empty #disappointed htt… \\n',\n",
       " \"honest image of a #violent #vengeful #needy #god ... #jesuischarlie #image _it's all about #perspective & #intention. \\n\",\n",
       " \"RT @Trailmixnarry: He's like an energetic five year old I love him so much \\n\",\n",
       " '@missuniversechina 2013 Jin Ye makes… \\n',\n",
       " 'Picture Frame #Distressed Red Black #Sentiment How Sweet It Is To Be #Loved By You #etsy \\n',\n",
       " 'RT @MagicLolitaxo: When you wake up and realize your life in Georgia is the most boring thing ever compared to @gordon_wild \\n',\n",
       " 'Cop Charged With Assault After Man Left Paralyzed \\n',\n",
       " 'RT @Paradoxy13: It seems the partial collapse of Aleppo Citadel wall was caused by regime bombing of a rebel tunnel, not vice versa. http:/… \\n',\n",
       " 'Antique WW1 Porcelain China Field Hospital Invalid Feeding Cup Red Cross Allied http://t.co/OPH0Yd0H4X? http://t.co/LqosTf5DTK\\n',\n",
       " '? http://t.co/6poORp0o7u\\n',\n",
       " 'Why are you feeling dispirited? Take the quiz: \\n',\n",
       " '@rjfapelacio nanggahot. Haha. :P #euro #impotent \\n',\n",
       " 'MAROONED BY TESLA! FIRES, POSSIBLE HACKING DEATHS, MULTIPLE DEFECTS. OWNERS?LAMENT https://t.co/hhEYzFNWh6 http://t.co/ljwtGdNJ43\\n',\n",
       " 'RT @BlackAurora_: My #drugs ??? http://t.co/mnUTa2ymlt\\n',\n",
       " 'Why do you get distressed so much? Take the quiz: http://t.co/csgAT6GyIZ http://t.co/wubmj6r2nZ\\n',\n",
       " 'Another @DaveMoran89 penalty flies past a helpless goalie (after bouncing twice) 3 out of 3 for him now #letissier \\n',\n",
       " \"RT @Amber_Montana: California's gloomy weather got me like https://t.co/DDaxSDGyUb \\n\",\n",
       " '(with a slight bit of fear) \\n',\n",
       " 'http://t.co/5fsXX9xTeV http://t.co/…\\n',\n",
       " 'RT @FriendsElderly: Loneliness is as big a risk to health as smoking. Help end #loneliness. Text BE A FRIEND to 70300 to donate ?3 \\n',\n",
       " '\"@zaynmalik: Wicked day yesterday :) xx delete \\n',\n",
       " '@ipaca_official @NTChoir great valentines treat singing all day with fellow enthusiastic teachers :-) \\n',\n",
       " '\"The State of the Comic Book Medium,\" a recent podcast from mit_cmsw: #comics \\n',\n",
       " 'RT @boredpanda: Gloomy Watercolor Cats Slowly Bleeding Into The Paper (12 pics): #watercolorpainting #cats \\n',\n",
       " 'RT @DanielleDonnely: hardly see me was TOO DARK! at the Red Carpet BFI Film vest, #wild / professional photo coming soon what got taken ? h… \\n',\n",
       " 'Trailers For Robert Kirkman’s Possession Series “Outcast”! #SDCC \\n',\n",
       " 'by youngwildanfree160 \"On vit mieux lorsque l\\'on attend rien de personne.\" #coup #de #blues #mal #moral #a #zero #I… \\n',\n",
       " 'RT @tobe_tobi: http://t.co/TileyCS31W\\n',\n",
       " 'Sissy made me oreo-filled pancakes!!!! Fattening but who could say no to these sinful indulgence??? Ehe \\n',\n",
       " 'RT @VoiceBosnia: A Spanish U.N. soldier in a destroyed street of Mostar, #Bosnia, 1994 | SEE MORE AT #History http:… \\n',\n",
       " '#LiveFromCzm by Photo by @karenzetina Lista para el baile?? #sobrina #carnaval #happy #enjoy #goodtime #cozumel #In… \\n',\n",
       " 'art by Cody Rocko http:… \\n',\n",
       " 'Ecstatic to be holding the first print copy of Wind In Your Sails. #biz #author \\n',\n",
       " 'Auth. Chanel light Beige Caviar Leather Zippy Long Wallet ~ Picts in the Descrp. - Full re… \\n',\n",
       " 'RT @Xbox: RT for a chance to win a @GearsofWar #XboxOne. #XboxSweepstakes #SDCC Rules \\n',\n",
       " '#5783 SK-II Color Clear Beauty Eye Shadow EYESHADOW 11 Sparkling NIB Japan \\n',\n",
       " '@Plowin_Donny Am I now? *Maribelle was taken aback, not exactly certain what he meant by how tooting she was.* \\n',\n",
       " 'RT @Allan_Galeano: Hey @TimGettys, check this out! Dream come true haha \\n',\n",
       " 'RT @LouiseArnold5: @SDDC if the bin men only empty half your bin-does that mean I only have to pay half my council tax! @DarrenArnold76 htt…\\n',\n",
       " 'RT @oldpicsarchive: WW1 submarine with a hull number of U-118 was found washed ashore on the beach at Hastings, Sussex, England, 1919. http… \\n',\n",
       " 'Hummus tahina @sweetgreen , Foggy Bottom, Washington, D.C. \\n',\n",
       " \"#earrings #Silver #Round #Medium #Cubic I really love these hoops. Haven't taken them off … \\n\",\n",
       " \"Thank you @WedFlowersMag for featuring Sally' designs in your March/April issue #delighted ? \\n\",\n",
       " 'RT @LanzaroteWorks: Delighted to present @grasshouseuk at @WaitingRoomN16 for a three night residency! https://t.co/tMjWHhiTP2 http://t.co/…\\n',\n",
       " 'RT @nnguerrero: Muy buena la historia de estos gemelos mezclados al nacer: The Mixed-Up Brothers of Bogotá \\n',\n",
       " 'Zenith bank Chairman, UBA disgraced.. CBN Governor might be in trouble for bringing Tony… http://t.co/H55NonXhf5 http://t.co/HF8bi0Zs07\\n',\n",
       " 'RT @Supreme_Tay: SHE REAL LIFE WRECKED THIS HO \\n',\n",
       " 'happy valentines day to me from me ?? \\n',\n",
       " '#BLF are kind of terrorists who cry loud after been beaten up by someone stronger thn em #AwaranBLFTerrorCampBusted http://t.co/XUZ0ZKBpjB\\n',\n",
       " 'This @MasterCard was one of my favs, then @CapitalOne bought it... #Worthless. Just you and me now, @AmericanExpress \\n',\n",
       " 'Before the #GRAMMYs check out @ImYoungChizz new debut #EP #Provoked now available on #iTunes https://t.co/TIJaE6SSiE \\n',\n",
       " '@belalandry clarification for the culturally inept \\n',\n",
       " \"Is the suspense killing you? Cause I'm pretty fucking pumped #overjoyed \\n\",\n",
       " '#アウディTT のユーザーが写真をアップ！ #auditt from instagram by ratty_tt \\n',\n",
       " '#abused #degraded #humiliated #humiliation #sm #hot #slave #sub - \\n',\n",
       " 'http://t.co/hmiVCMftC8 http://t.co/F6…\\n',\n",
       " 'The Cliffs of Moher in Ireland are what the sentiment of lacking words is meant for. \\n',\n",
       " 'WTF! Airline Passengers that DESERVE to be Shamed! http://t.co/mPKRMVWDKv http://t.co/ya2mXfgqTA\\n',\n",
       " '#pornstars #vintage #retro #pussy #licking #brunette #blowjob #wicked #xxxcom \\n',\n",
       " 'Passionate when dancing, and now singing. God bless you, Jongin. http://t.co/bd9jW90kgU\\n',\n",
       " 'RT @kikocosmeticsUK: Make your eyes pop with Miami Click Slick Eyeliner! A perfect, intense sweep of colour in just a \"click\"! \\n',\n",
       " 'Whaaaaa? #devastated ? RT @heatworld: Andrew Garfield dropped from Spider-Man films: \\n',\n",
       " \"I remember how she's so energetic when she tell me what she's wanna buy, she is so fussy girl \\n\",\n",
       " '@wasy_invalid и пфкцска занес – вот ведь гад! http://t.co/HCbj0djqsh\\n',\n",
       " 'Scary Abandoned Cities That Will Haunt Your Dreams \\n',\n",
       " 'RT @OTreeHill: \"When you find yourself lost in the darkness of despair, remember it\\'s only in the black of night you see the stars.\" http:/…\\n',\n",
       " '#February #Winter #Rainy #Stormy #Windy #Tuesday #Night #Love #Happy #Positive #Passionate #Calm #Joy #Fun #Coffee??? \\n',\n",
       " \"RT @TattedUpBreezy_: Bruhhhhh this my favorite episode of Wild N Out! Chico clowned Tf out of Safaree about meek and Nicki I'm dead ??? htt… \\n\",\n",
       " \"RT @800273TALK: We're on @Pinterest! Follow us for tips and advice on anxiety, depression, & more. #800273TALK http://t.co/QJkvrx9fJF http:…\\n\",\n",
       " 'RT @Free_Media_Hub: 300,000 Dead 1,200,000 injured 11,000,000 Displaced this is #Syria 2015 visit #US #Canada http… \\n',\n",
       " \".@theipaper Pls sign petition 1000's of Puppies tortured too for noodles! #StopBoknal2015 \\n\",\n",
       " 'when old men pay attention to you \\n',\n",
       " 'Cherishing Time by Vince Lusk \\n',\n",
       " 'ANTIQUE rare FOND PETS children Cloth book 1918 Saalfield Pub. Akron Ohio nice \\n',\n",
       " 'RT @PotterReacts: When your friend is petrified but you must get a cheeky photo http://t.co/FhEOsjIfzX\\n',\n",
       " 'Such an energetic group of dedicated teachers learning together @LoneStarTIA at 7:30 a.m in the summer. #tia15 http://t.co/kZOlHRhS8P\\n',\n",
       " 'RT @SkyUK: Desperate Housewives, Seasons 1-8 available now on Sky Box Sets. Which stage are you at? #BoxSetLife http://t.co/Dt6PmCn7lN\\n',\n",
       " 'Blog Post: #VivaBianca #Scorned #NudeScene #ShowingAss #Fetish #TiedUp #PosingHot #Beautiful \\n',\n",
       " 'In Argentina, an Anguished Anniversary for Jewish Center Bombing #Iran #IranDeal http://t.co/po8hFu9t5I http://t.co/I2rLo5sP3K\\n',\n",
       " 'Top Shot: Roads Aglow http://t.co/4DmHJEaFC6 http://t.co/RSmwxjmJzs #photography\\n',\n",
       " \"Damn @jes_chastain looks amazing and creepy. Can't wait to watch #CrimsonPeak I'm absolutely exhilarated \\n\",\n",
       " \"@westmeathcoco Some people just don't care. Our Countryside is destroyed . http://t.co/ND0Nh4WLoW\\n\",\n",
       " 'RT @LeakSnapchats: RT to help get the babies trapped in her knees out safely \\n',\n",
       " 'early valentines day gifts #whipped \\n',\n",
       " 'RT @Journalism2ls: “300 Awesome Free Tools (for your own News Startup)” —Voicester \\n',\n",
       " 'http://t.co/5TOGD52wrr http://t.co/8WcdOhFIzB\\n',\n",
       " 'RT @FloydMayweather: ....Miss Pac Man is broke and desperate for a pay day. Your Pay-Per-View numbers are a joke. ???? \\n',\n",
       " 'RT @TheSickDrawings: Bridge made out of an abandoned train-car. http://t.co/A6E7IDkLLc\\n',\n",
       " 'RT @GooleAFC: Tonight we recieved a donation of ?200 from our friends @GooleUnitedAFC towards the VPG defibrillator! #speechless \\n',\n",
       " 'RT @AlanStein: Many of the campers were misled into thinking I am famous. I devalued 100+ shirts this week from signing them. \\n',\n",
       " 'RT @ImPasky: Every black person KNOW these houses ?????? \\n',\n",
       " 'http://t.co/PZoCZVs7yg http://t.co/F2MslyKhJc #bookboost …\\n',\n",
       " 'Sexy Naked Amateur Girls, Beauty In Bra Lingerie 170 100% Real Amateurs Exposed! Naked ama… \\n',\n",
       " 'Acabou de vazar nova música do filme fast & furious \\n',\n",
       " 'Still feeling mildly disturbed by this fountain at Dom Jesus with the water coming out of its eyes... Seriously: why? \\n',\n",
       " 'Nobody appointed you the general spokesperson for #artisticcreativity @kanyewest #worthless #undeserved \\n',\n",
       " 'RT @LIONS4Mercy: FACT: Over 100 million animals are burned, crippled, poisoned, and abused in US labs every year. ? #BanAnimalTesting http:… \\n',\n",
       " 'This is happy good evening meet ; With such a fine scene sweet. \\n',\n",
       " 'RT @FoodPornPhotos: Hot Fudge & Whipped Pancakes. \\n',\n",
       " 'RT @MeloniFulvio: I got the power !!! By @melonifulvio #spoleto #umbria #italy #spirituality \\n',\n",
       " 'RT @UlrichJvV: 9-year-old Filipino boy photographed doing homework in the street light of a McDonald’s - so much wrong in our world! http:/… \\n',\n",
       " \"I swear niggas is Suckas ??? catch me lacking and they look down tilt they hat down and act like I don't exist \\n\",\n",
       " '#vibrant #bold #birthday #balloons in #bolton ?? \\n',\n",
       " \"It's all about little details. Thank you L #infatuated #gift #chivalryisnotdead \\n\",\n",
       " 'Just to be spiteful, @NaakiiChan. :D \\n',\n",
       " 'RT @ItsFoodPorn: Hot Chocolate with Whipped Cream \\n',\n",
       " 'RT @HoopVine: Basketball Level: Asian Dad \\n',\n",
       " '#20 #RT Desde Quando Você Se Foi, Fresno X Cartas Pra Você, NX Zero #FAV \\n',\n",
       " 'Ki Hong: \"Is that a video?\" ? \\n',\n",
       " 'RT @RevolutionSyria: Help the helpless. Tweet & retweet for #Syria. Don’t let evil get its way. #AssadHolocaust \\n',\n",
       " 'sums up my free time #exploring #adventuring #walking #train #tracks #traintracks #gloomy #grunge #hipster #punk \\n',\n",
       " '@chrisricewriter moderated lively & informative final panel on Sex in Thrillers with @RealCharlaine Lee Child et al. \\n',\n",
       " '\"@ThatKidRalph: I was going to be a child and violate butttt I\\'m grown #shook \\n',\n",
       " '\"#Desolate\" by pvallet67 - #D810 #France #Graffiti \\n',\n",
       " 'They Were Stunned When She Got Pregnant. But When They Saw the Sonogram... WOW \\n',\n",
       " \"RT @harpersbazaarus: Yes, you CAN cut your own bangs—here's how: \\n\",\n",
       " 'RT @Dolphin_Project: Wild dolphins swimming free in the Bahamas #DolphinProject \\n',\n",
       " \"RT @ItsRaniiiii: We got Tatum! It's a wrap. Will be shocked if Giles doesn't pick Duke now. #DukeNation #PackageDeal ??? \\n\",\n",
       " '#Fanghorn forest or #Alder #woodland up Glen #vorlich, #Lochearn. #petrified #forest #LOTR #Scotland \\n',\n",
       " 'Why do you get flustered so much? Take the quiz: \\n',\n",
       " 'GRACE ELEMENTS FROM THE USA WOMENS SLEEVELESS RUFFLE TOP IN BLACK - SIZE MEDIUM > #Womens… \\n',\n",
       " '????? ??. ??????? #Crazy #4MINUTE \\n',\n",
       " 'RT @rosegoldrican: happy Sunday yall? \\n',\n",
       " '#February #Winter #Rainy #Stormy #Windy #Tuesday #Night #Love #Happy #Positive #Passionate #Calm #Joy #Fun #Coffee??? \\n',\n",
       " '\"Maintain the memory and focus you need for a #vibrant life.\" - @TheRealMarilu. Try Spark! >>> \\n',\n",
       " '#me #Brooklyn #girl #gloomy #grunge #band #bass #darkness #music #musicians #live #cool #c… \\n',\n",
       " 'RT @autodesk: Inspired by student rooms this #render is a creepy interpretation of neglected sudent spaces. \\n',\n",
       " 'RT @Losquintana: ?nica función Madrid nuevo espectáculo #CRAZY 10 Marzo 22:00h @teatrolara NO TE LO PIERDAS!! http:/… \\n',\n",
       " 'RT @blackvoices: #BlackMuslimRamadan aims to celebrate a vibrant, often-neglected community \\n',\n",
       " 'RT @mental_runner: The best type of exercise for depression, anxiety & bipolar symptoms is ?: http://t.co/0CE7lm2bhT http://t.co/ne3SOKpORY\\n',\n",
       " 'You are allowed to terminate toxic relationships. #itsonus You are allowe #depression \\n',\n",
       " 'The pad for our new training facility  is nearly finished  and the building will be going up soon. http://t.co/n3MuulWbER\\n',\n",
       " 'Why are you feeling gloomy? Take the quiz: \\n',\n",
       " 'RT @IndySport: Lehmann must convince bewildered Australia it really was just a hiccup, says @bowlologist \\n',\n",
       " \"@TimdeLisle Better things for writer to work on.Not scathing enough. Not bdsm just dismal fantasy of this man's wife \\n\",\n",
       " 'Bad idea to open #bps today @marty_walsh Do you have any idea how #crippled we are in #DOT?!! This! For an hour!! \\n',\n",
       " \"Don't ever make someone else feel insignificant of devalued because of your twisted miscon… \\n\",\n",
       " 'RT @MinusIndustries: Day 365: #365PROJECT: 365 Days of 30 Minute Drawings #Yeg #YegArt #YegArts #TheEnd #End #Finished #Finale #Fin #MINUS … \\n',\n",
       " 'The Steel Curtain #thunderstruck #football #ilovepittsburgh #steelercountry \\n',\n",
       " 'Sparkling Blue Spinel gemstone heart Silver ring love handmade pr… https://t.co/FEhI6m04MG #bohojewelry #CuteJewelry \\n',\n",
       " '#Wiserswhiskeyquestionoftheday Cheerful Valentines Day Everybody ... Did You Get My Valentines Day Card \\n',\n",
       " 'RT @thenerdyteacher: Share your support on 7/14. End the stigma of Depression and Anxiety. #semicolonEDU #sunchat ht… \\n',\n",
       " '#vsco #vscocam #vscohub #vscobest #vscoonly #vscolife #vscolovers #vscofeel #instasize #instadaily #instaodessa #in… \\n',\n",
       " 'ht…\\n',\n",
       " 'Yayyyy papusas for birthday lunch?? 28yrs today #happybirthday #28 #papusas #mmmmm #foodie #foodporn #instafood #in… \\n',\n",
       " 'だいぶ眠い。仮眠とるか。#廃墟 #abandoned #ruins #haikyo #urbex #写真好きな人と繋がりたい \\n',\n",
       " 'Why are you feeling gloomy? Take the quiz: http://t.co/CSFllI7ZCU http://t.co/hh80OAgUcy\\n',\n",
       " 'RT @jimmurphySF: “If you run they will kill you so you just close your eyes so you don’t see the rapes,” http://t.co/d2sphLQ9oW http://t.co…\\n',\n",
       " 'RT @euphoriakay: the unholy trinity lol \\n',\n",
       " 'David Beckham: doting father with his little Harper! \\n',\n",
       " 'London swamped by #intolerant #uncompromising #belligerent #MuslimScum. #Britain is #dying & its #death is immanent \\n',\n",
       " 'Awww #MikeRalph You have #Delighted Me!!! #Cake ? @SodexoUK_IRE @Tech_Sodexo #MonthofLove ???? \\n',\n",
       " 'RT @SheeeRatchet: this episode would get me so heated, when Disney used to actually make sense. \\n',\n",
       " '#Guess Who This #Grinning #Little #Girl #Turned Into! \\n',\n",
       " 'RT @jennyschxo: Find something. I did this in school ? @NiallOfficial @Louis_Tomlinson #strong #LarryIsReal \\n',\n",
       " '#concrete #derelict #brutalist #brutalism #abandoned #London \\n',\n",
       " 'What is your #style like? #Simple and #elegant or #vibrant and #edgy? :) #fashion #girls #awesome #follow #amazing \\n',\n",
       " 'Cake is finished! Proud of it tbh so hey ho. ? \\n',\n",
       " 'I just defeated EGE in a duel! #letsDUAL \\n',\n",
       " '‘Fear The Walking Dead’ Sets August 23 Debut – Comic Con \\n',\n",
       " 'ARGHHHH!!!! NOOO!!! I was trying to wait for #Windows10 to upgrade my @nokia @LumiaUS 1020. #crushed #devastated \\n',\n",
       " 'Why do you get flustered so much? Take the quiz: http://t.co/j4RKAXai0s http://t.co/jTQqDLxYe5\\n',\n",
       " 'AMAZING SPARKLING RED BARYTE SPECIMEN ON CREATIVE CUSTOM MADE DISPLAY STAND \\n',\n",
       " '“ Because writer is too intimidated for asking to RP. ” \\n',\n",
       " 'Seriously when is @MrAshleyCain making his appearance on ExOnTheBeach2?. #Excited \\n',\n",
       " \"RT @UKinColombia: Delighted that @FALCAO is returning to the UK's @premierleague to play for @ChelseaFC #SportisGREAT @jeffglekin \\n\",\n",
       " 'RT @SpokespersonMoD: IAF rescues 28 marooned BSF Jawans from a Border Out Post near Ramgarh Village of Samba District in J&K. \\n',\n",
       " 'BEAUTY : #66781 NEW Mastex Professional Heated Beauty Booties Feet Therapy- White (MX-25) … \\n',\n",
       " 'RT @WhoSFX: Chris Barry chose this location as it looked desolate and reminded him of the surface of the moon. #TheSavages \\n',\n",
       " '#rwilly @rwillyofficial #hellagrip #smith #feeble #f… \\n',\n",
       " 'http://t.co/57QIx3053v http://t.co/ZNlfmKGL5H\\n',\n",
       " '\"@katiebennett_94: A picture speaks a thousand words #In WarbsWeTrust \\n',\n",
       " 'My #illustrations about the \"The Wrath of a #Robot Scorned.\" See more #book cover #art at \\n',\n",
       " '10 STARS WHO HAVE BEEN SHUNNED BY?HOLLYWOOD \\n',\n",
       " \"Kenda K1083A 26x2.35' Happy Medium Folding MTB Mountain Bicycle Tire - Black http://t.co/4Xbw3yUFJS http://t.co/6z5xX1UyYJ\\n\",\n",
       " '@vis_240 ?? ?? ???? ????? ??? ???????????????? ?? ? ??? \\n',\n",
       " 'RT @chrisashaffer: Went to SSC today to be a \"movie star\" to rep for the Deaf and got to meet an energetic great guy: Mark White! \\n',\n",
       " '@CooperativeFood pretty dismal that a supermarket - the co op no less - is hawking badly written porn. \\n',\n",
       " '@Merry__Can @HillaryClinton and she continues to be a liar \\n',\n",
       " 'RT @TravisMcEwanCBC: 4 units burned in a Morinville condo fire yesterday. Damage estimated at $1 million. Tenants not insured. #CBC \\n',\n",
       " \"RT @1DLiveReports: fun fact: Alberto Alvarez, Louis's body guard, used to be Michael Jackson's body guard -Lauren \\n\",\n",
       " 'USA BUSINESS DEALS : http://t.co/B2a7tkBv1s #9192 Executive Ergonomic Heated Vibrating Computer Desk Office Massa… http://t.co/Sgg4EnpjM3\\n',\n",
       " 'Colombia Road flower market. Good way to spend this gloomy Sunday. #sunflowers #SundayMarkets \\n',\n",
       " 'RT @whit0013: RT @McLovinGod: “Yeah, Drake ain’t tweet my album. Girl you know he don’t write his rhymes?” http://t.co/aOPWp6PpFM\\n',\n",
       " 'RT @DailySharky: @adamlevine is a devoted yoga student and will even bring his instructor on tour with him. #AdamLevineFacts http://t.co/PK…\\n',\n",
       " 'day 37 \"My full name is Cadence Sinclair Eastman.\" #finished #blisteringsmart #100happydays \\n',\n",
       " \"RT @ContentOutcast: My good friend's deepest fear. Part I. http://t.co/4RKCz9iOiL\\n\",\n",
       " 'A friend had her car damaged & bike stolen by some lowlifes last night in Kilkenny. Any sign online/elsewhere, DM me. http://t.co/PhYWDBZyg7\\n',\n",
       " '#20150609 CELEBUZZ: Um, Chris @prattprattpratt apparently didn\\'t know what \"\"impotent\"\" meant: http://t.co/9P77XdbuwR http://t.co/80K9tFflsY\\n',\n",
       " 'RT @RAFRed4: Great capture of the twister by @Fotomotion - this is taken just before I start to get dizzy! \\n',\n",
       " '#newpublicart Though personally opposed (and probably a little bitter) happy Valentines Day! #jeffkoons #publicart … \\n',\n",
       " 'Who wants a kiss ?? just crushed this workout http://t.co/e5QtxR8s8x\\n',\n",
       " 'Hows that corruption going for you this morning? Still going strong? \\n',\n",
       " 'How #the #Grammys #get #it #wrong #every #year #| #Grammys # #Grammys |PP \\n',\n",
       " 'That awkward moment when a random cat comes and lays in bed with you.... \\n',\n",
       " '@wejx_n ??? ????? ???? ?? ?? \\n',\n",
       " 'RT @FEMAgulag: ?SCARED, NEGLECTED 8yr spay #KITTY_KITTY Dump4NOTIME #NYC #adopt Upset \\n',\n",
       " 'wile Messiah taut His Set Apart Name.. \\n',\n",
       " 'http://t.co/BbCPU1r8pj\\n',\n",
       " 'Why are you feeling dismal? Take the quiz: \\n',\n",
       " '#empty #office #zombie #apocalypse by markt___ \\n',\n",
       " 'Why do you get distressed so much? Take the quiz: http://t.co/yu9r0ZlCyc http://t.co/b6jqTVMGvk\\n',\n",
       " \"RT @thinkprogress: How our flawed background check system put a gun in Dylann Roof's hands \\n\",\n",
       " 'Sprayed with iodine. They sent small replacement top and it fits! Even with an undershirt! #elated! \\n',\n",
       " '\"@_MusicForLovee: Happy Valentines Day Bby ???? ??????? \\n',\n",
       " 'A first: When when your 2-liter falls through the bag= 20 ft rocket launch, & huge puddle of pop. #baffled #bagfail \\n',\n",
       " 'RT @_ruggero: #TwitcamLodoRugg TT in Italia :) siete i migliori :) <3 http://t.co/k3AtPABk59\\n',\n",
       " 'A Black Republican wrote \"Lift Every Voice & Sing\" The Black Conservative \\n',\n",
       " 'Give me some money! #Hopeless \\n',\n",
       " 'Bad Blood Part Two? Taylor Swift and Nicki Minaj Have Heated Twitter Exhange Over MTV VMA Nod … http://t.co/b6HcWNLxcW\\n',\n",
       " \"But it's kind of Empty \\n\",\n",
       " 'Bit of bleakness. ? #bleak #blackandwhite #columns #trees \\n',\n",
       " 'A picture that shook the world. Read the #story and share it: #humanrights #starvation \\n',\n",
       " 'RT @Footy_Jokes: Mario finally has a smile on his face! \\n',\n",
       " '037 / 365 Engageante by ... - #Abandoned #Beauty #Carpet #Castle #Decay #Deserted \\n',\n",
       " 'RT @FutbolBible: BREAKING: Fenerbah?e have signed Robin van Persie from Manchester United. #RvP \\n',\n",
       " '[PIC] \" City boss Kevin Keegan appears dejected after United\\'s dominance. \" \\n',\n",
       " 'RT @NBCSports: Police: Bills coach Kromer punched a boy and threatened to kill his family over beach chairs. \\n',\n",
       " 'THIS remIX MAKES ME DIZZY \\n',\n",
       " 'I really can see love, peace and happiness in it. http://t.co/chKmzFOJdh\\n',\n",
       " 'Do you like #vampires?Forget #Twilight!\"The addiction\" 1995, #AbelFerrara, #movie about #good, #evil and #catharsis \\n',\n",
       " 'NAPOLEONIC FRENCH GENERAL GROUCHY 1812 - 54mm METAL PAINTED \\n',\n",
       " 'RT @LFCFrance: .@LucasLeiva87 parle des supporters des Reds du monde entier #LFC #LFCFrance \\n',\n",
       " '@thelionels ME? EXCITED!? Nooooo!! Nor is @camillashansen ? See You Tomorrow Boys! ?? http://t.co/cltZ8C6FPl\\n',\n",
       " 'RT @ViperSlays: @Chrissy_EU @Aqua1arda Here is your nan before I burned the cunt http://t.co/mbutJfNatw\\n',\n",
       " 'RT @ashleighperkin3: Plz RT for Ed guys! We r only in the lead by a few votes! ? #FanArmyFaceOff #Sheerios http://t.co/UlVfQ9ucel\\n',\n",
       " 'New on Ebay UK DAMAGED APPLE iPHONE 6 16GB SMARTPHONE (VODAFONE) - SPARES REPAIRS (RN349) http://t.co/agxuvkS2cK http://t.co/2EqJ3KmoFK\\n',\n",
       " \"I keep being startled, and thinking he's real! @AKJRiseley @AngelaSinclair \\n\",\n",
       " 'This thing works like crazy. I just finished brushing my two German Shepherds. @FURminator http://t.co/vDDQm8w3OY\\n',\n",
       " 'Chaotic Love - giclee print ?65 at #art #love #chaotic #abstract #blue #silver #prints #buy \\n',\n",
       " \"It's Time for a Change #WorldLoveForDolphinsDay \\n\",\n",
       " 'Mom Horrified After Young Son Finds Camera In Starbucks Toilet - \\n',\n",
       " '??? ??? ???? ?? ??? ???? ??? ??? ??8?8))) \\n',\n",
       " 'RT @neonmoonco: WHY ARE YOU SO OFFENDED? \\n',\n",
       " \"RT @RobertWRossEsq: William Bruce Ellis Ranken's ebullient portrait of the young Ernest Thesiger. Born this day in 1879. \\n\",\n",
       " 'This Bobby Brown lad is buoyant again! What a week for @OfficialIdeye ! Super performance today. Delighted for him! \\n',\n",
       " 'Peace, love and happiness to your hearts! SweetKisses? http… \\n',\n",
       " 'Abdel-Majed Abdel Bary: British jihadist deserted #Isis in #Syria and is on the run in... \\n',\n",
       " 'RT @JonHarvey1977: @TheRyanAdams Not an entirely enthusiastic response to Live at Carnegie Hall... \\n',\n",
       " 'I would like to take this time to show everyone my newest look. Props to the artist Juan Hernandez. .#crippled \\n',\n",
       " 'At -- #brooklyn #deserted #empty #cobblestone #street #night #nightshot #brooklynbridge #l… \\n',\n",
       " 'RT @NWSLosAngeles: Isolated showers and thunderstorms developing over the #Ventura and #SLO Mtns this evening. #CAwx http://t.co/kNlZPko0l1\\n',\n",
       " 'Black lives matter. Justice for Soror Sandra Bland. #sandyspeaks #sayhername #justiceforsandy http://t.co/Z4QcqfafAg\\n',\n",
       " 'RT @bookmyshow: #Disney\\'s lies - \"Every day is a good hair day\". Yeah, right!!: http://t.co/351AQVV7gA http://t.co/fFH59wGwbs\\n',\n",
       " '#RTした人で気になった人フォローする http…\\n',\n",
       " \"RT @_metafizik: Getting really incensed w/ the GOP's public displays of disrespect to the POTUS & America. Karma's due.#UniteBlue #P2 http:… \\n\",\n",
       " 'you can still conjointly match me up mentally \\n',\n",
       " '#StreetArtThrowdown Getting ready for disney night take two #disney #mickeymouse #excited #selfie \\n',\n",
       " 'Digiboo’s Zippy Download Kiosks Are Coming To An Airport Near You | Fast … – Fast?Company \\n',\n",
       " 'RT @Jessicalombao: Someone was overwhelmed... \\n',\n",
       " 'RT @The_Tolkienist: Moving house has a melancholy air.about it. It often means saying goodbye to a life you knew in exchange for a life… ht… \\n',\n",
       " '#Horrified #BMovie #Victims #Playset #Swagshop \\n',\n",
       " '7 for All Mankind A Pocket Womens Jeans Size 24 x 29 U130CJ080U distressed USA \\n',\n",
       " 'Blue leather bag, Women leather purse, Genuine leather, Women tote bag #etsymntt #etsyaaa \\n',\n",
       " 'We only use @simplysabuni products on our clients hair. Custom made #Haircare essentials! #Haircare #Hair #Damaged \\n',\n",
       " \"RT @InSightCrime: #Chapo's escape could paralyze .@EPN administration in terms of security policy. Our analysis http… \\n\",\n",
       " 'Pouring with rain but not downhearted. Huntingdon & Peterborough Federation Centenary Picnic. \\n',\n",
       " 'RT @TheFIREorg: Consent to sex at @CCUChanticleers must be enthusiastic, sober: https://t.co/ASQi4KMxYd http://t.co/ngZIe3l8fQ\\n',\n",
       " 'RT @BreeNewsome: History will haunt this nation like a vengeful spirit until it does right by my people. #BlackLivesMatter #McKinney http:/… \\n',\n",
       " \"Seen on Fahlo:Cut or don't cut? ?? \\n\",\n",
       " 'RT @TotalPolitics: Furious Labour backlash as Harriet Harman says party will back Tory welfare reforms \\n',\n",
       " '@TrippyPip use only if desperate \\n',\n",
       " 'Ridge Avenue is closed after a partial building collapse and electrical fire Saturday night: \\n',\n",
       " 'Just sat down on the plane! #shook #a380 \\n',\n",
       " '? 2015 Harold Green. http://t.co/8nlcPhr66W\\n',\n",
       " ' #Namjuhyuk #??? #namjoohyuk http…\\n',\n",
       " '#KanyeWest #Kanye #KimKardashian #NorthWest #PornStar #Trash #LoL #Kanyeisadouchebag #kanyeisanidiot #desperate \\n',\n",
       " 'RT @YouChoices: Mountains or sea? \\n',\n",
       " '#ganja #ganjalife #ganjah #marijuana #weedstagram #stoned #smoke #cannabis #dope #kush #liveloud #thc #weedporn #in… \\n',\n",
       " '#destroyed grill Know God Free Twitter Traffic \\n',\n",
       " 'RT @john_kucko: Storms miss @rochester, Lake Ontario sky aglow this evening @spann @JimCantore @spensgen #ItsAmazingOutThere http://t.co/08…\\n',\n",
       " 'RT @undercoverbun: Happiness ;() http://t.co/XIWZNpjM6w\\n',\n",
       " 'Aquatic Hitchhiker [Digipak] * by Leftover Salmon (CD, May-2012, LoS Records) via eBay \\n',\n",
       " 'RT @NHAparty: Be passionate about a podiatrist #loveNHS \\n',\n",
       " 'First watermelon eating contest win w/ 1.25 watermelons (5 pieces) in 3 minutes. \\n',\n",
       " 'Winners will be chosen in 1 week. \\n',\n",
       " \"RT @AHappyFlower: Apologies for relative silence. I've been marooned in 1947 #filming #Fenland @ousewasheslp \\n\",\n",
       " 'The barbarity of vivisection can never be part of a modern compassionate society #BoycottCambridge http://t.co/jiD7bTZ9w4\\n',\n",
       " '#Repost @elislime with @repostapp.?????#безудержноевеселье #упороты #кисы #india #goa #travel #crazy by araukaria.3 \\n',\n",
       " 'The Golden star @NawalElZoghbi in @Prada black bag #Fashion #NawalElZoghbi . \\n',\n",
       " 'you #belong to #God. #Always try to do what is #right,be #devoted to #God, & have #faith, Have a #Blessed #Monday \\n',\n",
       " 'Just Give it to #Jesus he will take care of it #JesusCalling #Depression #follow \\n',\n",
       " 'Why are you feeling despondent? Take the quiz: http://t.co/X8enSTDais http://t.co/rqGpV7QFfs\\n',\n",
       " 'Feeling weepy. Marcee brings me cake & tea. Does this make me part of #blessed twitter or... \\n',\n",
       " 'RT @AusOpinion: Paul Kelly: The Abbott government now lives on borrowed time. #libspill #auspol \\n',\n",
       " 'Genuine HP 21 Black Ink Cartridge in Damaged Retail Box http://t.co/7uDACIfDMf http://t.co/UxqRbq7Dg4\\n',\n",
       " '#24themovie http://t.co/91TxrS3eq6\\n',\n",
       " 'Train carriage all to myself today! #offpeak #publictransport #empty #train #queenslandrai… \\n',\n",
       " 'When u sit on the quad forgetting it was exposed to sunlight. ???? \\n',\n",
       " '@tawlondon administer a #dazzling extravaganza of #euphoric printed garments for #SS15 \\n',\n",
       " 'Ariana Grande Has Made Lisa Rinna Fearful of Donuts For Good Reason! #RHOBH #DonutGate \\n',\n",
       " 'RT @theblendergirl: Always trying to mix up my #KALE SALADS & am loving this southwestern version @VeganYackAttack h… \\n',\n",
       " \"Captivity? No Thanks! He'll empty ALL those tanks ! @SimonCowell #Cowell4Dolphins \\n\",\n",
       " 'RT @thefader: Premiere: Listen to @iamkevingates buoyant but X-rated new song, \"Kno One.\" \\n',\n",
       " 'RT @KateJBryan: The Judges have judged! #wrap #landscapeartistoftheyear @SkyArts You are in for a treat!! \\n',\n",
       " 'RT @arialvv: \"@JackmerOfficial: Crazy Idea (uh la la) [Lyric Video] https://t.co/BsOGYaIH7F ME GUSTAAA \\n',\n",
       " 'A little more persistence, a little more effort, and what seemed #hopeless #failure may turn to #glorious #success \\n',\n",
       " '\"live the full life of the mind, exhilarated by new #ideas, intoxicated by the romance of … http://t.co/7m3yLxmJ2G http://t.co/hSfHJJh6xz\\n',\n",
       " 'My favorite: crowds of realtors waiting for doors to open #hangry #crushed \\n',\n",
       " 'Just learned Girl Scout cookies are now $5 per box. #APPALLED #DISBELIEF #INFLATION #FormerGirlScout \\n',\n",
       " \"RT @SamWalkerOBX: Well played RVA'ers, well played indeed. They were poking fun at all of the Panicky Pete's & hype of sharks on #OBX http:…\\n\",\n",
       " 'RT @_lucyandbelle: To celebrate the arrival of #Zippy do you want to #win a Fun Bandana Gift Set? Just RT and follow us! Ends 13.02.15! htt… \\n',\n",
       " '? the shocked ones http://t…\\n',\n",
       " 'RT @mylilguppy: @TheGilbert23Mom @JVM Valerie found her babies Carson Daisy like a discarded pile of garbage #DogsMatter #Gilbert23 http://…\\n',\n",
       " 'Why are you feeling dismal? Take the quiz: http://t.co/VNiVdWdbP5 http://t.co/PBNJS8Ciqr\\n',\n",
       " 'RT @corcorgiBH: ? ???? ???? ??? ?? ???????? ?? ?? ??? ??? ??? \"??? ?? ?????\" ??? ??? ??????????????? http://t.co/0JADQVn44B\\n',\n",
       " 'RT @Distinctboxes: Palmerston Island, Cook Islands -  one of the most isolated parts of the Pacific Ocean. http://t.co/bma9pi2qgV\\n',\n",
       " '#AssadHolocaust the world grieved at the burning of asoldier and they silence at the burning of hundreds of civilians \\n',\n",
       " 'RT @southernpride50: SAFE 2/10/15. ADOPTED. TY. HAPPINESS LILY??SUPER URGENT 2/10/15.NYC. https://t.co/gbD7DshAkT. \\n',\n",
       " 'RT @AnimalBibIe: A little circle of happiness ? \\n',\n",
       " '3x retweets … as per attached : \\n',\n",
       " 'RT @micknugent: Please retweet to support Irish teachers excluded from state-funded schools because they are atheist \\n',\n",
       " 'RT @minnymooo: Haha maknae keep falling while squatting, so this is what they do ? think her high cut sneaker makes it hard to squat http:/…\\n',\n",
       " 'RT @sohailakhan48: the macho man ? fitness freak #intense #light #1YearOfParamSinghOnTwitter @8paramsingh loveallord… \\n',\n",
       " '#Enthusiastic Mr & Mrs Grundling drives away in their BMW X5 @Zambesi_Auto @LegendsZambesi @ZambesiMotorrad \\n',\n",
       " 'Why are you feeling desolate? Take the quiz: http://t.co/FRaBQTw2Ej http://t.co/jSzhn5CLev\\n',\n",
       " 'laney wont stop bleeding. ?? \\n',\n",
       " 'Whenever you see some heated drama unfolding on your timeline… \\n',\n",
       " '????????????????????? ???????????????????????? http://t.co/eDK8ixMI7v\\n',\n",
       " '@4evarandom @GlambeR5t Ninetails are intelligent and vengeful, they are protective of their trainers and do not stand \\n',\n",
       " 'A fabulous evening with @The_Kingsolver . Thanks to @Hollard for making a very special evening possible. #elated \\n',\n",
       " 'RT @Vicente56Juan: Confusión en carnaval. \\n',\n",
       " 'Lana always looks super thrilled to take pictures with me ? \\n',\n",
       " 'RT @jungkookreally: BTS FOREVER GETTING INTIMIDATED BY YOONGI http://t.co/GeYyn5zfeI\\n',\n",
       " 'RT @BlSCUlTS: weird Twitter has ruined my sense of humor, I laughed at this for 10 mins \\n',\n",
       " 'Authentic Pandora Sparkling Stiletto Pendant Charm 791536CZ - Full read by eBay \\n',\n",
       " 'How a #bitcoin atm promotes mixed martial arts in a #lasvegas gym #crypto #cryptocurrency \\n',\n",
       " 'RT @BlkHistStudies: One Love & Unity! \\n',\n",
       " 'We asked @rebeccaminkoff for the one word that describes her state of mind leading up to her show... #EXHILARATED #… \\n',\n",
       " '@bexmader I hope you like my drawing of Zelena ? Lots of love! #wicked \\n',\n",
       " 'RT @IrishTimes: Peru to make contact with isolated tribe for first time http://t.co/QJXtXML03Y via @IrishTimesWorld http://t.co/bM0jQ6ABCg\\n',\n",
       " 'RT @raewalther: Long lost pals ?? \\n',\n",
       " 'RT @camilacabows: \"my therapist taught me to start thinking of my anxiety as my panicky friend. it’s working,\" \\n',\n",
       " \"Watching Titanic on Valentine's Day while eating a crap ton of chocolate... I'm just... So ecstatic. Can't you tell?? \\n\",\n",
       " 'This is what my business studies turn into everytime... #hopeless @yelyahwilliams @schzimmydeanie @paramore \\n',\n",
       " 'Happy Steph, Sad Steph, Angry Steph, Sympathetic Steph ? \\n',\n",
       " \"RT @seasaltclifford: @weirdnumber5SOS the fact that calum can't buy decent cigarettes hurt \\n\",\n",
       " 'RT @ReactionBeyonce: Twelve years ago today, Beyoncé earned her first solo No.1 for Crazy In Love on Billboard Hot 100! \\n',\n",
       " \"Check out #CITIZENSOFHUMANITY Women's #Daisy #Distressed #Relaxed #TaperedLeg #Jeans http://t.co/ZbNnNViN9X via @eBay http://t.co/SsjhmkHY3e\\n\",\n",
       " 'by beni43 @ Hachi Rokus... Hachi Rokus #everywhere. :) Just #finished the #ae86. #handmade… \\n',\n",
       " 'RT @girlideas: when u accidentally hurt ur pet \\n',\n",
       " \"RT @dreamsskinny82: Why can't I b like this & then people might actually want 2 b with me! #thinspo #ednos #anamia #ugly #depression http:/… \\n\",\n",
       " 'RT @OilyRagClothing: Our range of Carl Fogarty (official) Tshirts #Foggy #Ducati #King ?@carlfogarty available http… \\n',\n",
       " 'And in this one, GEJ seems like someone deserted & confused >>> \\n',\n",
       " 'RT @emitoms: Foggy drives ? \\n',\n",
       " '\"If you are not outraged then you are not listening\" \\n',\n",
       " 'Behold the lilies of the field.....#bible #caring @lajlaj1 @CSPressRoom \\n',\n",
       " 'RT @peta2: There’s nothing “entertaining” about seeing a helpless animal in captivity. #NotOurs \\n',\n",
       " \"I can't even tell you guys how much I love these covers ? @Melissa_Landers #invaded #alienated #prettycovers \\n\",\n",
       " 'Greeted by strange children at the atm #antibanter #frightened #poverty #sorrow #yourbalanceis13cent #familyfun \\n',\n",
       " '#trapped #trampoline #cool #frozen #freeze #freezing #frost #frosty #dinosaur #prehistoric #awesome #amazing #cold … \\n',\n",
       " 'RT @TWooodley: Tonight he shocked the world! @thenotoriousmma @ufc #UFC189 #UFCFightWeek #ufc @afflictionclothing \\n',\n",
       " \"Finished In HOURS! - SIMPLE to Supercharge Your Marketing. Don 't Miss... http://t.co/7iCqcCz8QN http://t.co/CUqLMfHpCr\\n\",\n",
       " 'RT @MBfashionweek: Shades of mermaid backstage at Son Jung Wan #MBFW | #NYFW \\n',\n",
       " 'Do you ever feel like the guy on the left!? #Monday #gloomy #difficultpeople #flockoff \\n',\n",
       " 'Crushed it at the @WarriorDash in Minnesota yesterday! #WarriorNation \\n',\n",
       " 'Someone needs to explain to me what is going on in this picture. #befuddled #crazy \\n',\n",
       " 'RT @choosgin: #FlexibleWorking: working from home drives #productivity 13% more but #loneliness is downside \\n',\n",
       " 'RT @Patriots: Patriots Football Weekly takes a look at the defensive ends: \\n',\n",
       " 'RT @FootyVibez: All I need for valentines is the happiness of playing soccer ?? \\n',\n",
       " 'Used 2011 Holiday Rambler Augusta 25PCS Class C Motor Home For Sale.Reduced $10K - Full re… \\n',\n",
       " 'RT @EmWhitham: Instagram has banned #CURVY. The reason why will infuriate you. It certainly infuriated me: https://t.co/Xtxg12TsLW http://t…\\n',\n",
       " 'RT @AUMTAENY: ?????????????????????? ???????????????????????????????????????!!! 2000 ???????? ????????????????????????????? 5555 http://t.c…\\n',\n",
       " '#KimKPrada this is just tacky. There is nothing fashionable here. These companies use her as a novelty. #disgraced \\n',\n",
       " 'Man, someone was a little tired after our go at practice today #canthang #broken @DabeGean \\n',\n",
       " \"RT @yildraws: imagining mitsutada as that smart-looking student & ookurikara as the delinquent _(:'3 J #tourabu69min \\n\",\n",
       " \"#ski #resort @ #akita #prefecture's #tazawako #area are beautiful & ... #empty! Go while this lasts! #powder #snow \\n\",\n",
       " 'RT @JessikaDommeUK: Come and visit my interrogation and punishment cell. #caged #beaten #hooded \\n',\n",
       " 'Sometimes music, inspires me. #broken-lyric. \\n',\n",
       " \"RT @DailyMirror: 'Drunk man is beaten and tied up by fellow plane passengers' http://t.co/m9JidWkDET http://t.co/ricnLmYfZe\\n\",\n",
       " 'How I feel after I #lift! The #pump is #strong in me! #FITFAM #TheStruggleIsReal #ForceChoke \\n',\n",
       " \"RT @simonstacpoole: Cracking Popperfoto pic on @MailSport shows a dejected Ayrton #Senna at Silverstone '89. #picturetellsathousandwords ht… \\n\",\n",
       " \"RT @NotebookMsgs: It's the birthday of a man who made our childhoods awesome just by being in it! Happy Birthday Robin Williams! http://t.c…\\n\",\n",
       " 'All Natural Bandage 4 #Cooks! STOPS #Bleeding in SECONDS #culinary #recipes @rachelray @gzchef \\n',\n",
       " 'RT @rdonadon: Poche ore dalla partenza questa splendida due giorni di brain-storming e ci sono prime vittime #hackwine @zonin1821 \\n',\n",
       " 'RT @Wimbledon: With help from @toddwoodbridge, we break down where the final was won & lost: #Wimbledon \\n',\n",
       " 'RT @GhostTown: New @bmthofficial song ?? \\n',\n",
       " 'New Driver Power Towing Mirror Heat Heated for 04-13 Nissan Titan Pickup Truck http://t.co/4PoBJbYUgQ http://t.co/sLAeglcAZa\\n',\n",
       " '@gallowsofficial just pre ordered \"desolate sounds\".... now to play the waiting game \\n',\n",
       " 'Why buy #ordinary art when you can have your art on #sailcloth. #vibrant colors!! \\n',\n",
       " 'RT @Nitsuano: Oh ya big thanks to @mr_mortified again for this amazing glasses holder. \\n',\n",
       " 'RT @Telegraph: Crippled Greece yields to overwhelming power as deal looms | @AmbroseEP \\n',\n",
       " 'Just finished this oak dresser. One of my favorites. http://t.co/5B8TX1CDDa\\n',\n",
       " \"betterfeelingfilms: RT via Instagram: First day of filming #powerless back in 2011. Can't … \\n\",\n",
       " '??? #stunned #sunglasses #gafas #gafasdesol \\n',\n",
       " 'Seeing @nbdbnb in a few days #excited #or #terrified #California #food \\n',\n",
       " 'And the #dragon guarding our #heart is called #fear... \\n',\n",
       " 'RT @NWSAlbuquerque: Isolated strong to severe storms developing over western NM early this afternoon. #nmwx \\n',\n",
       " 'Cliff dwelling above Peacocks Gallop (recently demolished) #abandoned #streetphotography #earthquake #Christchurch \\n']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff849d8",
   "metadata": {},
   "source": [
    "### 生成数据字典。\n",
    "\n",
    "这段代码定义了一个名为 `create_dict` 的函数，其目的是根据给定的文本数据文件生成一个字符到整数映射的字典，并将该字典保存到指定路径。\n",
    "\n",
    "\n",
    "调用这个函数时，提供了数据文件路径 `all_data_path=\"all_data.txt\"` 和字典输出路径 `dict_path=\"dict.txt\"`。运行此函数将会基于 `all_data.txt` 文件中的内容生成一个字符到整数的编码字典，并将字典内容保存到 `dict.txt` 文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61a62ad3-45b6-4cbb-878b-f28e91d103b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-27T07:40:13.134691Z",
     "iopub.status.busy": "2023-06-27T07:40:13.134306Z",
     "iopub.status.idle": "2023-06-27T07:40:13.159976Z",
     "shell.execute_reply": "2023-06-27T07:40:13.159161Z",
     "shell.execute_reply.started": "2023-06-27T07:40:13.134663Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据字典生成完成！\n"
     ]
    }
   ],
   "source": [
    "# 生成数据字典\n",
    "def create_dict(data_path, dict_path):\n",
    "    with open(dict_path, 'w') as f:\n",
    "        f.seek(0)\n",
    "        f.truncate() \n",
    "\n",
    "    dict_set = set()\n",
    "    # 读取全部数据\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    # 把数据生成一个元组\n",
    "    for line in lines:\n",
    "        content = line.split('\\t')[-1].replace('\\n', '')\n",
    "        for s in content:\n",
    "            dict_set.add(s)\n",
    "    # 把元组转换成字典，一个字对应一个数字\n",
    "    dict_list = []\n",
    "    i = 0\n",
    "    for s in dict_set:\n",
    "        dict_list.append([s, i])\n",
    "        i += 1\n",
    "    # 添加未知字符\n",
    "    dict_txt = dict(dict_list)\n",
    "    end_dict = {\"<unk>\": i}\n",
    "    dict_txt.update(end_dict)\n",
    "    end_dict = {\"<pad>\": i+1}\n",
    "    dict_txt.update(end_dict)\n",
    "    # 把这些字典保存到本地中\n",
    "    with open(dict_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(str(dict_txt))\n",
    "        \n",
    "    print(\"数据字典生成完成！\")\n",
    "\n",
    "all_data_path=\"all_data.txt\"\n",
    "dict_path=\"dict.txt\"\n",
    "create_dict(all_data_path,dict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9757d39",
   "metadata": {},
   "source": [
    "##### f_write_txt的理解\n",
    "\n",
    "`f_write_txt` 函数的目的是将一个单词列表 `words` 转换为序列化表示形式，并附加给定的标签 `label`，然后返回格式化的字符串。该函数不涉及任何预测逻辑，而是对已知数据进行编码和格式化处理。\n",
    "\n",
    "所以，根据这个函数的功能，它不能用于预测未知标签数据，因为它只负责对已有标签的数据进行编码和格式化输出。对于预测任务，通常会有一个模型训练过程，并使用训练好的模型来对新的、未标记的数据进行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd923612-482e-4e82-b5ea-bce39481179f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-27T07:43:40.006001Z",
     "iopub.status.busy": "2023-06-27T07:43:40.004841Z",
     "iopub.status.idle": "2023-06-27T07:43:40.116368Z",
     "shell.execute_reply": "2023-06-27T07:43:40.115109Z",
     "shell.execute_reply.started": "2023-06-27T07:43:40.005954Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n",
      "无法在词汇表中找到字符 位，已跳过\n",
      "无法在词汇表中找到字符 言，已跳过\n",
      "无法在词汇表中找到字符 細，已跳过\n",
      "无法在词汇表中找到字符 調，已跳过\n",
      "无法在词汇表中找到字符 整，已跳过\n",
      "无法在词汇表中找到字符 理，已跳过\n",
      "无法在词汇表中找到字符 球，已跳过\n",
      "无法在词汇表中找到字符 ω，已跳过\n",
      "无法在词汇表中找到字符 ぁ，已跳过\n",
      "无法在词汇表中找到字符 奇，已跳过\n",
      "无法在词汇表中找到字符 跡，已跳过\n",
      "无法在词汇表中找到字符 ョ，已跳过\n",
      "无法在词汇表中找到字符 位，已跳过\n",
      "无法在词汇表中找到字符 ←，已跳过\n",
      "无法在词汇表中找到字符 ，已跳过\n",
      "无法在词汇表中找到字符 ユ，已跳过\n",
      "无法在词汇表中找到字符 ぶ，已跳过\n",
      "无法在词汇表中找到字符 仮，已跳过\n",
      "无法在词汇表中找到字符 ォ，已跳过\n",
      "数据列表生成完成！\n",
      "141\n",
      "141\n",
      "141\n"
     ]
    }
   ],
   "source": [
    "# 创建序列化表示的数据\n",
    "def load_vocab(file_path):\n",
    "    fr = open(file_path, 'r', encoding='utf8')\n",
    "    vocab = eval(fr.read())   #读取的str转换为字典\n",
    "    fr.close()\n",
    "    return vocab\n",
    "\n",
    "# 返回包含了编码序列(将字转化为其编码)和标签的格式化字符串。\n",
    "\n",
    "\n",
    "def f_write_txt(words, dict_txt, label):\n",
    "    labs = \"\"\n",
    "    \n",
    "    # for s in words:\n",
    "    #     lab = str(dict_txt[s])\n",
    "    #     labs = labs + lab + ','\n",
    "    # labs = labs[:-1]\n",
    "    # labs = labs + '\\t\\t\\t\\t\\t' + label + '\\n'\n",
    "    # return labs\n",
    "\n",
    "    for s in words:\n",
    "        try:\n",
    "            lab = str(dict_txt[s])\n",
    "            labs = labs + lab + ','\n",
    "        except KeyError as e:\n",
    "            print(f\"无法在词汇表中找到字符 {s}，已跳过\")\n",
    "            continue\n",
    "    labs = labs[:-1]\n",
    "    labs = labs + '\\t\\t\\t\\t\\t' + label + '\\n'\n",
    "    return labs\n",
    "\n",
    "def create_data_list(data_path, train_path, val_path, last_path,real_path, dict_path):\n",
    "    \n",
    "    dict_txt = load_vocab(dict_path)\n",
    "    #print(dict_txt)\n",
    "    with open(data_path, 'r', encoding='utf-8') as f_data:\n",
    "        lines = f_data.readlines()\n",
    "        print(len(lines))\n",
    "        #print(lines)\n",
    "    \n",
    "    i = 0\n",
    "    maxlen = 0\n",
    "    with open(real_path, 'w', encoding='utf-8') as f_real:\n",
    "        for line in contents:\n",
    "        # for line in lines:\n",
    "            \n",
    "            words = line.split('\\t')[-1].replace('\\n', '')\n",
    "            # print(words)\n",
    "            maxlen = max(maxlen, len(words))\n",
    "            #label = line.split('\\t')[0]\n",
    "            # label是标签\n",
    "            label=str(labels[i])\n",
    "            # print(labels[i])\n",
    "            labs = f_write_txt(words, dict_txt, label)\n",
    "            # print(labs)\n",
    "\n",
    "            f_real.write(labs)\n",
    "            i += 1\n",
    "\n",
    "    i2 = 0\n",
    "    maxlen2 = 0\n",
    "    with open(train_path, 'w', encoding='utf-8') as f_train, \\\n",
    "         open(val_path, 'w', encoding='utf-8') as f_val:\n",
    "        for line2 in contents:\n",
    "        # for line in lines:\n",
    "            \n",
    "            words2 = line2.split('\\t')[-1].replace('\\n', '')\n",
    "            #print(words)\n",
    "            maxlen2 = max(maxlen2, len(words2))\n",
    "            #label = line.split('\\t')[0]\n",
    "            label2=str(labels[i2])\n",
    "            labs2 = f_write_txt(words2, dict_txt, label2)\n",
    "            # 每8个 抽取一个数据用于验证\n",
    "            if i2 % 8 == 0:\n",
    "                f_val.write(labs2)\n",
    "            else:\n",
    "                f_train.write(labs2)\n",
    "            i2 += 1\n",
    "    \n",
    "    i1 = 0\n",
    "    maxlen1 = 0\n",
    "    with open(last_path, 'w', encoding='utf-8') as f_last:\n",
    "        for line1 in contents1:\n",
    "        # for line in lines:\n",
    "            \n",
    "            words1 = line1.split('\\t')[-1].replace('\\n', '')\n",
    "            # print(words1)\n",
    "            maxlen1 = max(maxlen1, len(words1))\n",
    "            #label = line.split('\\t')[0]\n",
    "            label1=str(labels1[i1])\n",
    "            # print(labels1[i1])\n",
    "            labs1 = f_write_txt(words1, dict_txt, label1)\n",
    "            # 每8个 抽取一个数据用于验证\n",
    "            # if i % 8 == 0:\n",
    "            #     f_val.write(labs)\n",
    "            # else:\n",
    "            f_last.write(labs1)\n",
    "            i1 += 1\n",
    "    print(\"数据列表生成完成！\")\n",
    "    print(maxlen)\n",
    "    print(maxlen1)\n",
    "    print(maxlen2)\n",
    "\n",
    "\n",
    "train_path=\"train_data.txt\"\n",
    "val_path=\"val_data.txt\"  # 需要先定义验证集路径\n",
    "last_path=\"last_data.txt\"\n",
    "real_path=\"real_data.txt\"\n",
    "# data_path就是all_data_path，即训练集的所有文本\n",
    "create_data_list(all_data_path, train_path, val_path,last_path,real_path, dict_path)\n",
    "\n",
    "\n",
    "\n",
    "# def create_data_list(data_path, train_path, val_path, dict_path):\n",
    "#     dict_txt = load_vocab(dict_path)\n",
    "#     maxlen = 0\n",
    "\n",
    "#     with open(data_path, 'r', encoding='utf-8') as f_data:\n",
    "#         lines = f_data.readlines()\n",
    "#         total_count = len(lines)\n",
    "#         validation_split = 0.2\n",
    "\n",
    "#     # 初始化训练和验证文件的写入句柄\n",
    "#     with open(train_path, 'a', encoding='utf-8') as f_train, \\\n",
    "#          open(val_path, 'a', encoding='utf-8') as f_val:\n",
    "\n",
    "#         for i, line in enumerate(lines):\n",
    "#             words = line.split('\\t')[-1].replace('\\n', '')\n",
    "#             maxlen = max(maxlen, len(words))\n",
    "#             label = str(labels[i])  # 假设 labels 已经正确初始化或提取\n",
    "\n",
    "#             labs = f_write_txt(words, dict_txt, label)\n",
    "\n",
    "            # 判断当前行是否属于验证集部分\n",
    "#             if i >= total_count * (1 - validation_split):  # 最后20%的数据划分为验证集\n",
    "#                 f_val.write(labs)\n",
    "#             else:\n",
    "#                 f_train.write(labs)\n",
    "\n",
    "#     print(\"数据列表生成完成！\")\n",
    "#     print(maxlen)\n",
    "\n",
    "# train_path=\"train_data.txt\"\n",
    "# val_path=\"val_data.txt\"  # 需要先定义验证集路径\n",
    "# create_data_list(all_data_path, train_path, val_path, dict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b640b30",
   "metadata": {},
   "source": [
    "### 基于PaddlePaddle框架的数据加载器，用于处理一个多模态（文本+图像）的任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be0cafb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "=============train_dataset =============\n",
      "Tensor(shape=[3, 224, 224], dtype=float32, place=Place(cpu), stop_gradient=True,\n",
      "       [[[0.        , 0.01176471, 0.02352941, ..., 0.00784314,\n",
      "          0.01568628, 0.        ],\n",
      "         [0.00784314, 0.02352941, 0.02352941, ..., 0.02352941,\n",
      "          0.02745098, 0.        ],\n",
      "         [0.00392157, 0.10588236, 0.17254902, ..., 0.04313726,\n",
      "          0.02352941, 0.00392157],\n",
      "         ...,\n",
      "         [0.00392157, 0.10980393, 0.17254902, ..., 0.08235294,\n",
      "          0.04313726, 0.01176471],\n",
      "         [0.01176471, 0.02352941, 0.03137255, ..., 0.04705883,\n",
      "          0.03529412, 0.        ],\n",
      "         [0.        , 0.01568628, 0.03921569, ..., 0.02745098,\n",
      "          0.02352941, 0.        ]],\n",
      "\n",
      "        [[0.23137257, 0.20000002, 0.19215688, ..., 0.20784315,\n",
      "          0.19607845, 0.21960786],\n",
      "         [0.23137257, 0.18431373, 0.15686275, ..., 0.19607845,\n",
      "          0.20000002, 0.21960786],\n",
      "         [0.19607845, 0.22352943, 0.25098041, ..., 0.16862746,\n",
      "          0.18431373, 0.21176472],\n",
      "         ...,\n",
      "         [0.19607845, 0.21568629, 0.22352943, ..., 0.15686275,\n",
      "          0.18039216, 0.20392159],\n",
      "         [0.21960786, 0.17647059, 0.14509805, ..., 0.17647059,\n",
      "          0.19215688, 0.21176472],\n",
      "         [0.24313727, 0.20784315, 0.20392159, ..., 0.19607845,\n",
      "          0.18823531, 0.21568629]],\n",
      "\n",
      "        [[0.21176472, 0.21176472, 0.21960786, ..., 0.21568629,\n",
      "          0.19607845, 0.20000002],\n",
      "         [0.22352943, 0.20392159, 0.18823531, ..., 0.21568629,\n",
      "          0.20392159, 0.20392159],\n",
      "         [0.20392159, 0.25490198, 0.29019609, ..., 0.19607845,\n",
      "          0.19215688, 0.20000002],\n",
      "         ...,\n",
      "         [0.18431373, 0.23137257, 0.24705884, ..., 0.17254902,\n",
      "          0.18431373, 0.19215688],\n",
      "         [0.19607845, 0.18431373, 0.16862746, ..., 0.18823531,\n",
      "          0.19215688, 0.19215688],\n",
      "         [0.21176472, 0.21176472, 0.22745100, ..., 0.20392159,\n",
      "          0.18823531, 0.19215688]]])\n",
      "[ 76 627  95  28 363  38 163 118  27   1 363 119 119 125 385 118 558 309\n",
      " 363  92 181 627 628 668 668 125 118  92 627 119 119 363  38 163 118 682\n",
      " 309 668   1 668 118 125  28 668 668  27 363 668 682 385 118 668  38  27\n",
      " 309  52 125 363 627 125  27 363  92 118 627  27 309 119 668  27 668 125\n",
      " 385 118  49 119  52 668 118 125 181 420 118 215 118 125  38 350  28 118\n",
      " 627  27 118 510  52 627 119  49 668   1  27 627 152 627   1 356 118 510\n",
      " 395  14 119  49 668   1  27 627 118 685 685 685 685 685 685 685 685 685\n",
      " 685 685 685 685 685 685 685 685 685 685 685 685 685 685 685 685 685 685\n",
      " 685 685 685 685 685 685]\n",
      "(150,)\n",
      "[1]\n",
      "=============val_dataset =============\n",
      "[ 98 637 118 167  14 356 363  27 161  28 627 356 363 321 321 369 118 637\n",
      " 309 668 118  92 350  38 125 588 363   1 627  27 350   1 125 118 309 627\n",
      " 310 668 118 627  38 118 668 310 363 119 118 668 420 668 118 215 118 627\n",
      "   1 668 118  38 350  28 118 125 668  27 118  27 350 118 588 309 420 125\n",
      " 363  92 627 119 119 420 118 627  27  27 627  92 181 118  14 125 627   1\n",
      " 627 356 118  90 627 588  52 118 401 363 436 118 510  76 668 271 668 356\n",
      " 627  38 628 161 627 152 668  27 420 659  90 627 588  52 448 363 118 309\n",
      "  27  27 588 369 335 335  27 294  92 350 335 449 442 421 685 685 685 685\n",
      " 685 685 685 685 685 685]\n",
      "(150,)\n",
      "[0]\n",
      "=============last_dataset =============\n",
      "Tensor(shape=[3, 224, 224], dtype=float32, place=Place(cpu), stop_gradient=True,\n",
      "       [[[0.        , 0.01176471, 0.02352941, ..., 0.00784314,\n",
      "          0.01568628, 0.        ],\n",
      "         [0.00784314, 0.02352941, 0.02352941, ..., 0.02352941,\n",
      "          0.02745098, 0.        ],\n",
      "         [0.00392157, 0.10588236, 0.17254902, ..., 0.04313726,\n",
      "          0.02352941, 0.00392157],\n",
      "         ...,\n",
      "         [0.00392157, 0.10980393, 0.17254902, ..., 0.08235294,\n",
      "          0.04313726, 0.01176471],\n",
      "         [0.01176471, 0.02352941, 0.03137255, ..., 0.04705883,\n",
      "          0.03529412, 0.        ],\n",
      "         [0.        , 0.01568628, 0.03921569, ..., 0.02745098,\n",
      "          0.02352941, 0.        ]],\n",
      "\n",
      "        [[0.23137257, 0.20000002, 0.19215688, ..., 0.20784315,\n",
      "          0.19607845, 0.21960786],\n",
      "         [0.23137257, 0.18431373, 0.15686275, ..., 0.19607845,\n",
      "          0.20000002, 0.21960786],\n",
      "         [0.19607845, 0.22352943, 0.25098041, ..., 0.16862746,\n",
      "          0.18431373, 0.21176472],\n",
      "         ...,\n",
      "         [0.19607845, 0.21568629, 0.22352943, ..., 0.15686275,\n",
      "          0.18039216, 0.20392159],\n",
      "         [0.21960786, 0.17647059, 0.14509805, ..., 0.17647059,\n",
      "          0.19215688, 0.21176472],\n",
      "         [0.24313727, 0.20784315, 0.20392159, ..., 0.19607845,\n",
      "          0.18823531, 0.21568629]],\n",
      "\n",
      "        [[0.21176472, 0.21176472, 0.21960786, ..., 0.21568629,\n",
      "          0.19607845, 0.20000002],\n",
      "         [0.22352943, 0.20392159, 0.18823531, ..., 0.21568629,\n",
      "          0.20392159, 0.20392159],\n",
      "         [0.20392159, 0.25490198, 0.29019609, ..., 0.19607845,\n",
      "          0.19215688, 0.20000002],\n",
      "         ...,\n",
      "         [0.18431373, 0.23137257, 0.24705884, ..., 0.17254902,\n",
      "          0.18431373, 0.19215688],\n",
      "         [0.19607845, 0.18431373, 0.16862746, ..., 0.18823531,\n",
      "          0.19215688, 0.19215688],\n",
      "         [0.21176472, 0.21176472, 0.22745100, ..., 0.20392159,\n",
      "          0.18823531, 0.19215688]]])\n",
      "[ 46  38 668   1 163 668  27 363  92 118  27   1 627 363  38 363  38 163\n",
      " 118  27 350 628 627 420 118  28 363  27 309 118 350  52   1 118 161 627\n",
      "  38 118  14  38  27 350  38 363 350 118 449 668  28 118 271 350 119 119\n",
      " 627   1 125 335 449 668  28 118  64 627   1  27  38 668   1 125 118  27\n",
      "   1 627 363  38 668 668 125 118 685 685 685 685 685 685 685 685 685 685\n",
      " 685 685 685 685 685 685 685 685 685 685 685 685 685 685 685 685 685 685\n",
      " 685 685 685 685 685 685 685 685 685 685 685 685 685 685 685 685 685 685\n",
      " 685 685 685 685 685 685 685 685 685 685 685 685 685 685 685 685 685 685\n",
      " 685 685 685 685 685 685]\n",
      "(150,)\n",
      "[8]\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "from paddle.nn import Conv2D, Linear, Embedding\n",
    "from paddle import to_tensor\n",
    "import paddle.nn.functional as F\n",
    "import os, zipfile\n",
    "import io, random, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(paddle.__version__)\n",
    "from PIL import Image\n",
    "# 加载词汇\n",
    "vocab=load_vocab(\"dict.txt\")\n",
    "import sys\n",
    "\n",
    "from paddle.vision import transforms \n",
    "\n",
    "# # 定义训练集增强算子\n",
    "# train_transforms = T.Compose([\n",
    "#     T.RandomCrop(crop_size=224),\n",
    "#     T.RandomHorizontalFlip(),\n",
    "#     T.Normalize()])\n",
    "\n",
    "\n",
    "class RumorDataset(paddle.io.Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.all_data = []\n",
    "        self.images=images\n",
    "        self.transform = transform\n",
    "       \n",
    "        with io.open(self.data_dir, \"r\", encoding='utf8') as fin:\n",
    "            for line in fin:\n",
    "                cols = line.strip().split(\"\\t\\t\\t\\t\\t\")\n",
    "                if len(cols) != 2:\n",
    "                    sys.stderr.write(\"[NOTICE] Error Format Line!\")\n",
    "                    continue\n",
    "                label = []\n",
    "                label.append(int(cols[1]))\n",
    "                wids = cols[0].split(\",\")\n",
    "                if len(wids)>=150:\n",
    "                    wids = np.array(wids[:150]).astype('int64')     \n",
    "                else:\n",
    "                    wids = np.concatenate([wids, [vocab[\"<pad>\"]]*(150-len(wids))]).astype('int64')\n",
    "                label = np.array(label).astype('int64')\n",
    "                self.all_data.append((wids, label))\n",
    "\n",
    "        \n",
    "    # def __getitem__(self, index):\n",
    "    #     data, label = self.all_data[index]\n",
    "    #     image=self.images[index]\n",
    "    #     #print(len(self.images),len(self.all_data))\n",
    "    #     #root=\"./dataset/data\"\n",
    "    #     #path=os.path.join(root,image)\n",
    "    #     img = Image.open(image)\n",
    "\n",
    "    #     # Resize the image to 512*412\n",
    "    #     img = img.resize((224, 224))\n",
    "    #     img = transforms.ToTensor()(img)\n",
    "    #     return img,data, label\n",
    "    def __getitem__(self, index):\n",
    "        # print(self.images)\n",
    "        if index < 0 or index >= len(self.images):\n",
    "            raise IndexError(f\"Index {index} is out of range for the dataset.\")\n",
    "\n",
    "        data, label = self.all_data[index]\n",
    "        image_path = self.images[index]\n",
    "\n",
    "        # Ensure the image exists and can be opened\n",
    "        try:\n",
    "            img = Image.open(image_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: The file '{image_path}' was not found. Returning a placeholder.\")\n",
    "            img = Image.new('RGB', (224, 224), color=(0, 0, 0))\n",
    "\n",
    "        # Resize the image to 224x224\n",
    "        img = img.resize((224, 224))\n",
    "\n",
    "        # Apply any transformations, if defined\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # Convert the image to a tensor\n",
    "        img = transforms.ToTensor()(img)\n",
    "\n",
    "        return img, data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = RumorDataset(train_path)\n",
    "val_dataset = RumorDataset(val_path)\n",
    "last_dataset = RumorDataset(last_path)\n",
    "\n",
    "train_loader = paddle.io.DataLoader(train_dataset, return_list=True,\n",
    "                                     batch_size=batch_size, drop_last=True,num_workers=0)\n",
    "test_loader = paddle.io.DataLoader(val_dataset, return_list=True,\n",
    "                                     batch_size=batch_size, drop_last=True,num_workers=0)\n",
    "last_loader = paddle.io.DataLoader(last_dataset, return_list=True,\n",
    "                                     batch_size=batch_size, drop_last=True,num_workers=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#check\n",
    "\n",
    "print('=============train_dataset =============') \n",
    "for image,data, label in train_dataset:\n",
    "    print(image)\n",
    "    print(data)\n",
    "    print(np.array(data).shape)\n",
    "    print(label)\n",
    "    break\n",
    "\n",
    "\n",
    "print('=============val_dataset =============') \n",
    "for image,data, label in val_dataset:\n",
    "    print(data)\n",
    "    print(np.array(data).shape)\n",
    "    print(label)\n",
    "    break\n",
    "\n",
    "print('=============last_dataset =============') \n",
    "for image,data, label in last_dataset:\n",
    "    print(image)\n",
    "    print(data)\n",
    "    print(np.array(data).shape)\n",
    "    print(label)\n",
    "    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e994a52e",
   "metadata": {},
   "source": [
    "# 模型定义"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd926d2",
   "metadata": {},
   "source": [
    "### 多模态混合模型实现（单模态在最底部）\n",
    "\n",
    "下述代码在PaddlePaddle中定义了CNN类，还包含了两种类型的残差块：Basicblock和Bottleneckblock\n",
    "\n",
    "在`Basicblock`类：\n",
    "- 该块包含三个Conv2D层和三个BatchNorm2D层。\n",
    "- 它按照顺序执行卷积、归一化和ReLU激活函数操作。\n",
    "- 提供了处理步长为2的情况，此时快捷连接可能也需要对输入进行下采样。\n",
    "\n",
    "在`Bottleneckblock`类：\n",
    "- 该块与Basicblock相似，但增加了一个卷积层，并采用了瓶颈设计，其中内部卷积层具有较少的通道数。\n",
    "- 它也处理了作为ResNet架构中新阶段起始块的情况。\n",
    "\n",
    "在`CNN`类：\n",
    "- 初始化了包括Embedding层、Conv2D、MaxPool2D、Linear层等在内的多种层，并根据`bottlenet`参数决定是否使用Basicblocks或Bottleneckblocks。\n",
    "- `add_basic_layer`和`add_bottleneck_layer`方法分别创建基本或瓶颈残差块的序列组。\n",
    "- 在前向传播过程中，模型通过独立路径处理图像和文本两种输入。对于图像路径，它应用一系列卷积、池化以及残差层操作；对于文本路径，它对输入进行嵌入，接着应用卷积和池化操作，然后将两条路径上展平后的特征拼接起来，最后通过一个最终的线性层（`hidden5`）生成输出结果。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4f4b201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "from paddle.nn import Conv2D, MaxPool2D, AdaptiveAvgPool2D, Linear, ReLU, BatchNorm2D\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "class Basicblock(paddle.nn.Layer):\n",
    "    def __init__(self, in_channel, out_channel, stride = 1):\n",
    "        super(Basicblock, self).__init__()\n",
    "        self.stride = stride\n",
    "        self.conv0 = Conv2D(in_channel, out_channel, 3, stride = stride, padding = 1)\n",
    "        self.conv1 = Conv2D(out_channel, out_channel, 3, stride=1, padding = 1)\n",
    "        self.conv2 = Conv2D(in_channel, out_channel, 1, stride = stride)\n",
    "        self.bn0 = BatchNorm2D(out_channel)\n",
    "        self.bn1 = BatchNorm2D(out_channel)\n",
    "        self.bn2 = BatchNorm2D(out_channel)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        y = inputs\n",
    "        x = self.conv0(inputs)\n",
    "        x = self.bn0(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        if self.stride == 2:\n",
    "            y = self.conv2(y)\n",
    "            y = self.bn2(y)\n",
    "        z = F.relu(x+y)\n",
    "        return z\n",
    "\n",
    "class Bottleneckblock(paddle.nn.Layer):\n",
    "    def __init__(self, inplane, in_channel, out_channel, stride = 1, start = False):\n",
    "        super(Bottleneckblock, self).__init__()\n",
    "        self.stride = stride\n",
    "        self.start = start\n",
    "        self.conv0 = Conv2D(in_channel, inplane, 1, stride = stride)\n",
    "        self.conv1 = Conv2D(inplane, inplane, 3, stride=1, padding=1)\n",
    "        self.conv2 = Conv2D(inplane, out_channel, 1, stride=1)\n",
    "        self.conv3 = Conv2D(in_channel, out_channel, 1, stride = stride)\n",
    "        self.bn0 = BatchNorm2D(inplane)\n",
    "        self.bn1 = BatchNorm2D(inplane)\n",
    "        self.bn2 = BatchNorm2D(out_channel)\n",
    "        self.bn3 = BatchNorm2D(out_channel)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        y = inputs\n",
    "        x = self.conv0(inputs)\n",
    "        x = self.bn0(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        if self.start:\n",
    "            y = self.conv3(y)\n",
    "            y = self.bn3(y)\n",
    "        z = F.relu(x+y)\n",
    "        return z\n",
    "\n",
    "# class Resnet(paddle.nn.Layer):\n",
    "#     def __init__(self, ):\n",
    "#         super(Resnet, self).__init__()\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "# def resnet18():\n",
    "#     return Resnet()\n",
    "\n",
    "#定义卷积网络\n",
    "class CNN(paddle.nn.Layer):\n",
    "    def __init__(self,num, bottlenet):\n",
    "        super(CNN,self).__init__()\n",
    "        self.dict_dim = vocab[\"<pad>\"]\n",
    "        self.emb_dim = 128\n",
    "        self.hid_dim = 128\n",
    "        self.fc_hid_dim = 96\n",
    "        self.class_dim = 2\n",
    "        self.channels = 1\n",
    "        self.win_size = [3, self.hid_dim]\n",
    "        self.batch_size = 32\n",
    "        self.seq_len = 150\n",
    "        self.embedding = Embedding(self.dict_dim + 1, self.emb_dim, sparse=False)\n",
    "        self.hidden1 = paddle.nn.Conv2D(in_channels=1,                        #通道数\n",
    "                                            out_channels=self.hid_dim,        #卷积核个数\n",
    "                                            kernel_size=self.win_size,        #卷积核大小\n",
    "                                            padding=[1, 1]\n",
    "                                            )                         \n",
    "        self.relu1 = paddle.nn.ReLU()\n",
    "        self.hidden3 = paddle.nn.MaxPool2D(kernel_size=2,         #池化核大小\n",
    "                                            stride=2)             #池化步长2\n",
    "        self.hidden4 = paddle.nn.Linear(128*75, 512)\n",
    "\n",
    "        self.conv0 = Conv2D(3, 64, 7, stride=2)\n",
    "        self.bn = BatchNorm2D(64)\n",
    "        self.pool1 = MaxPool2D(3, stride=2)\n",
    "        if bottlenet:\n",
    "            self.layer0 = self.add_bottleneck_layer(num[0], 64, start = True)\n",
    "            self.layer1 = self.add_bottleneck_layer(num[1], 128)\n",
    "            self.layer2 = self.add_bottleneck_layer(num[2], 256)\n",
    "            self.layer3 = self.add_bottleneck_layer(num[3], 512)\n",
    "        else:\n",
    "            self.layer0 = self.add_basic_layer(num[0], 64, start = True)\n",
    "            self.layer1 = self.add_basic_layer(num[1], 128)\n",
    "            self.layer2 = self.add_basic_layer(num[2], 256)\n",
    "            self.layer3 = self.add_basic_layer(num[3], 512)\n",
    "        self.pool2 = AdaptiveAvgPool2D(output_size = (1, 1))\n",
    "        self.hidden5 = paddle.nn.Linear(512, 3)\n",
    "\n",
    "    def add_bottleneck_layer(self, num, inplane, start = False):\n",
    "        layer = []\n",
    "        if start:\n",
    "            layer.append(Bottleneckblock(inplane, inplane, inplane*4, start = True))\n",
    "        else:\n",
    "            layer.append(Bottleneckblock(inplane, inplane*2, inplane*4, stride = 2, start = True))\n",
    "        for i in range(num-1):\n",
    "            layer.append(Bottleneckblock(inplane, inplane*4, inplane*4))\n",
    "        return nn.Sequential(*layer)\n",
    "\n",
    "    #def forward(self, inputs):\n",
    "       \n",
    "\n",
    "    def add_basic_layer(self, num, inplane, start = False):\n",
    "        layer = []\n",
    "        if start:\n",
    "            layer.append(Basicblock(inplane, inplane))\n",
    "        else:\n",
    "            layer.append(Basicblock(inplane//2, inplane, stride = 2))\n",
    "        for i in range(num-1):\n",
    "            layer.append(Basicblock(inplane, inplane))\n",
    "        return nn.Sequential(*layer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #网络的前向计算过程\n",
    "    def forward(self,inputs,input):\n",
    "        \n",
    "        #print('输入维度：', input.shape)\n",
    "        x = self.embedding(input)\n",
    "        x = paddle.reshape(x, [32, 1, 150, 128])   \n",
    "        x = self.hidden1(x)\n",
    "        x = self.relu1(x)\n",
    "        #print('第一层卷积输出维度：', x.shape)\n",
    "        x = self.hidden3(x)\n",
    "        #print('池化后输出维度：', x.shape)\n",
    "        #在输入全连接层时，需将特征图拉平会自动将数据拉平.\n",
    "\n",
    "        x = paddle.reshape(x, shape=[self.batch_size, -1])\n",
    "        out1 = self.hidden4(x)\n",
    "\n",
    "\n",
    "\n",
    "        x = self.conv0(inputs)\n",
    "        x = self.bn(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)        \n",
    "        x = self.pool2(x)\n",
    "        x = paddle.squeeze(x)\n",
    "        out=out1+x\n",
    "        out=self.hidden5(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fea398a",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ed5fb6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 接着定义了`train`函数来训练模型。\n",
    "\n",
    "在这个函数中：\n",
    "\n",
    "1. 首先将模型设置为训练模式（model.train()）。\n",
    "2. 初始化Adam优化器，学习率为0.0002，并将模型的所有参数传递给优化器。\n",
    "3. 初始化一些变量以记录训练进度，包括总步数(steps)、迭代次数列表(Iters)、累计损失列表(total_loss)和累计准确率列表(total_acc)。\n",
    "4. 进行2个训练epoch循环：\n",
    "   - 对每个训练批次进行以下操作：\n",
    "     - 从数据加载器(train_loader)中获取当前批次的图像(image)、文本特征(sent)和标签(label)。\n",
    "     - 将输入送入模型计算预测logits。\n",
    "     - 计算交叉熵损失(cross_entropy)。\n",
    "     - 计算准确率(accuracy)。\n",
    "     - 每50个批次更新一次Iters、total_loss和total_acc，并打印当前epoch和batch_id下的损失值。\n",
    "     - 反向传播损失以计算梯度，并通过调用optimizer.step()更新模型权重，之后清空梯度缓存(opt.clear_grad())。\n",
    "   - 在每个epoch结束时评估模型在验证集上的表现：\n",
    "     - 使用test_loader遍历验证数据集，计算平均准确率(avg_acc)和平均损失(avg_loss)，但这里应该使用测试集而不是验证集，因为代码里没有明确划分。\n",
    "     - 打印验证集上的平均准确率和损失。\n",
    "     - 再次将模型切换回训练模式(model.train())以继续下一个epoch的训练。\n",
    "\n",
    "最后，在完成所有训练后，保存模型的权重到文件\"model_final.pdparams\"。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb66352c-9a26-471b-a097-d1b85cbb6aec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-27T07:47:32.925168Z",
     "iopub.status.busy": "2023-06-27T07:47:32.924219Z",
     "iopub.status.idle": "2023-06-27T08:43:24.276597Z",
     "shell.execute_reply": "2023-06-27T08:43:24.275419Z",
     "shell.execute_reply.started": "2023-06-27T07:47:32.925134Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch_id: 0, loss is: 1.0466840267181396,acc is: 0.5625\n",
      "epoch: 0, batch_id: 50, loss is: 1.2147316932678223,acc is: 0.4375\n",
      "epoch: 0, batch_id: 100, loss is: 1.0031020641326904,acc is: 0.5\n",
      "[validation] accuracy: 0.42916667461395264, loss: 1.0121403932571411\n",
      "epoch: 1, batch_id: 0, loss is: 0.8420470952987671,acc is: 0.5625\n",
      "epoch: 1, batch_id: 50, loss is: 0.9547401070594788,acc is: 0.59375\n",
      "epoch: 1, batch_id: 100, loss is: 0.754788875579834,acc is: 0.625\n",
      "[validation] accuracy: 0.5708333253860474, loss: 0.9572482705116272\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def train(model):\n",
    "    model.train()\n",
    "    opt = paddle.optimizer.Adam(learning_rate=0.002, parameters=model.parameters())\n",
    "    steps = 0\n",
    "    Iters, total_loss, total_acc = [], [], []\n",
    "    for epoch in range(2):\n",
    "        # 块名称\n",
    "        for batch_id, data in enumerate(train_loader):\n",
    "            try:\n",
    "                steps += 1\n",
    "                image = data[0]\n",
    "                sent = data[1]\n",
    "                label = data[2]\n",
    "                # 根据图像和文本预测的数据\n",
    "                logits = model(image,sent)\n",
    "                # print(logits)\n",
    "                # 损失的定义（交叉熵损失函数）\n",
    "                loss = paddle.nn.functional.cross_entropy(logits, label)\n",
    "                # print(\"loss\",loss)\n",
    "                acc = paddle.metric.accuracy(logits, label)\n",
    "                # print(\"acc\",acc)\n",
    "                if batch_id % 50 == 0:\n",
    "                    Iters.append(steps)\n",
    "                    # 原先代码\n",
    "                    # total_loss.append(loss.numpy()[0])\n",
    "                    # print(loss.numpy())\n",
    "                    # print(loss.numpy()[0])\n",
    "                    # total_acc.append(acc.numpy()[0])\n",
    "                    total_loss.append(loss.numpy())\n",
    "                    total_acc.append(acc.numpy())\n",
    "                    print(\"epoch: {}, batch_id: {}, loss is: {},acc is: {}\".format(epoch, batch_id, loss.numpy(),acc.numpy()))\n",
    "                \n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                opt.clear_grad()\n",
    "            \n",
    "            except IndexError:\n",
    "                print(f\"Warning: Encountered an indexing error at epoch {epoch}, batch_id {batch_id}. Skipping this batch.\")\n",
    "                continue\n",
    "\n",
    "        # evaluate model after one epoch\n",
    "        model.eval()\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        for batch_id, data in enumerate(test_loader):\n",
    "            try:\n",
    "                # sent = data[0]\n",
    "                # #print(type(sent))\n",
    "                # label = data[1]\n",
    "                # logits = model(sent)\n",
    "                image=data[0]\n",
    "                sent = data[1]\n",
    "                label = data[2]\n",
    "                logits = model(image,sent)\n",
    "                loss = paddle.nn.functional.cross_entropy(logits, label)\n",
    "                acc = paddle.metric.accuracy(logits, label)\n",
    "                accuracies.append(acc.numpy())\n",
    "                losses.append(loss.numpy())\n",
    "            except IndexError:\n",
    "                print(f\"Warning: Encountered an indexing error at epoch {epoch}, batch_id {batch_id}. Skipping this batch.\")\n",
    "                continue\n",
    "            \n",
    "        avg_acc, avg_loss = np.mean(accuracies), np.mean(losses)\n",
    "        print(\"[validation] accuracy: {}, loss: {}\".format(avg_acc, avg_loss))\n",
    "        model.train()\n",
    "\n",
    "    paddle.save(model.state_dict(),\"model_final.pdparams\")\n",
    "\n",
    "        \n",
    "model=CNN([2,2,2,2], bottlenet = False)\n",
    "train(model)\n",
    "\n",
    "# text_model = TextModel(len(vocab))\n",
    "# # train_and_evaluate_text_only(text_, text_loader)\n",
    "# num_blocks = [2,2,2,2]  # 或者从配置文件、命令行参数等地方获取\n",
    "\n",
    "# image_model = ImageModel(num_blocks,bottlenet = True)\n",
    "# train_and_evaluate_image_only(image_model, image_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fccd192",
   "metadata": {},
   "source": [
    "### 查看数据加载器加载了多少批次（对确认最后一批次是否被遗漏）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f4ad8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_loader实际加载了15个批次的数据。\n"
     ]
    }
   ],
   "source": [
    "total_batches_loaded = 0\n",
    "for _ in last_loader:\n",
    "    total_batches_loaded += 1\n",
    "\n",
    "print(f\"last_loader实际加载了{total_batches_loaded}个批次的数据。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb9ed0b",
   "metadata": {},
   "source": [
    "# 模型预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bfa039",
   "metadata": {},
   "source": [
    "- 对于test_without_label预测结果，生成test1.txt（过程文件）\n",
    "- 执行模型推理，处理测试集中的样本，生成预测结果，并将这些结果以及真实的标签写入到指定的文本文件中，以便后续评估或分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "368c4c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {str(i): label for i, label in enumerate(['negative', 'neutral', 'positive'])}\n",
    "path2label1 = {}\n",
    "\n",
    "with open('dataset/test1.txt', 'w') as f:\n",
    "    for batch_id, data in enumerate(last_loader):\n",
    "        image_paths, input_sents, true_labels = data[0], data[1], data[2]\n",
    "\n",
    "        logits = model(image_paths, input_sents)\n",
    "        probs = F.softmax(logits, axis=-1)\n",
    "        pred_label_indices = paddle.argmax(probs, axis=1)\n",
    "\n",
    "        real_labels = true_labels.numpy().astype(int).flatten()\n",
    "\n",
    "        # 遍历批次中的每个样本\n",
    "        for i in range(pred_label_indices.shape[0]):\n",
    "            pred_label = class_dict[str(pred_label_indices[i].numpy())]\n",
    "            image_path = image_paths[i]\n",
    "\n",
    "            # 更新字典并将预测结果写入文件\n",
    "            path2label1[image_path] = pred_label\n",
    "            f.write(f\"{real_labels[i]},{pred_label}\\n\")\n",
    "\n",
    "# 注意：确保您的model前向传播计算输出的是针对批次内每个样本的logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba42041",
   "metadata": {},
   "source": [
    "### 预测数据整合\n",
    "- 对于最后一个不满足32的批次数据进行处理，生成updates_test2.txt文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d05f0bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_test2(test1_path, test2_path):\n",
    "    # 读取并解析test1的内容到字典中，键为第一列字符串，值为第二列字符串\n",
    "    mapping_dict = {}\n",
    "    with open(test1_path, 'r') as file1:\n",
    "        for line in file1:\n",
    "            key, value = line.strip().split(',')\n",
    "            mapping_dict[key] = value\n",
    "\n",
    "    # 读取test2的内容，根据mapping_dict更新第二列，并写入新的内容到新的或原test2文件中\n",
    "    new_test2_content = []\n",
    "    with open(test2_path, 'r') as file2:\n",
    "        for line in file2:\n",
    "            key_in_test2, _ = line.strip().split(',')\n",
    "            if key_in_test2 in mapping_dict:\n",
    "                new_line = f\"{key_in_test2},{mapping_dict[key_in_test2]}\"\n",
    "            else:\n",
    "                new_line = f\"{key_in_test2},neutral\"\n",
    "            new_test2_content.append(new_line)\n",
    "\n",
    "    # 写回更新后的内容（这里假设我们创建一个新的文件来保存结果，也可以选择覆盖原文件）\n",
    "    output_path = 'dataset/updated_test2.txt'  # 输出文件路径\n",
    "    with open(output_path, 'w') as outfile:\n",
    "        for line in new_test2_content:\n",
    "            outfile.write(f\"{line}\\n\")\n",
    "\n",
    "# 使用函数更新test2文件\n",
    "update_test2('dataset/test1.txt', 'dataset/test2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c4f776",
   "metadata": {},
   "source": [
    "# 模型消融对比"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc582852",
   "metadata": {},
   "source": [
    "### 生成仅有图片的模型\n",
    "\n",
    "这段代码定义了一个基于PaddlePaddle框架的图像分类模型（ImageModel），它可以根据参数`bottlenet`选择使用BottleneckBlock或Basicblock构建残差网络结构，包含卷积、批量归一化、最大池化以及自适应平均池化等层，并通过一个全连接层将特征映射到指定数量的类别输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ddd5403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ImageModel(paddle.nn.Layer):\n",
    "    def __init__(self, num_blocks, bottlenet=False, class_dim=2):  # 添加class_dim参数\n",
    "        super(ImageModel, self).__init__()\n",
    "        \n",
    "        self.conv0 = Conv2D(3, 64, 7, stride=2)\n",
    "        self.bn = BatchNorm2D(64)\n",
    "        self.pool1 = MaxPool2D(3, stride=2)\n",
    "\n",
    "        if bottlenet:\n",
    "            self.layer0 = self.add_bottleneck_layer(num_blocks[0], 64, start=True)\n",
    "            self.layer1 = self.add_bottleneck_layer(num_blocks[1], 128)\n",
    "            self.layer2 = self.add_bottleneck_layer(num_blocks[2], 256)\n",
    "            self.layer3 = self.add_bottleneck_layer(num_blocks[3], 512)\n",
    "        else:\n",
    "            self.layer0 = self.add_basic_layer(num_blocks[0], 64, start = True)\n",
    "            self.layer1 = self.add_basic_layer(num_blocks[1], 128)\n",
    "            self.layer2 = self.add_basic_layer(num_blocks[2], 256)\n",
    "            self.layer3 = self.add_basic_layer(num_blocks[3], 512)\n",
    "\n",
    "        self.pool2 = AdaptiveAvgPool2D(output_size=(1, 1))\n",
    "        self.hidden5 = paddle.nn.Linear(512, class_dim)  # 修改为正确的输入维度和类别数量\n",
    "\n",
    "    def add_bottleneck_layer(self, num, inplane, start=False):\n",
    "        layer = []\n",
    "        if start:\n",
    "            layer.append(BottleneckBlock(inplane, inplane, inplane*4, start=True))\n",
    "        else:\n",
    "            layer.append(BottleneckBlock(inplane, inplane*2, inplane*4, stride=2, start=start))\n",
    "        for _ in range(num - 1):\n",
    "            layer.append(BottleneckBlock(inplane, inplane*4, inplane*4))\n",
    "        return nn.Sequential(*layer)\n",
    "    \n",
    "    def add_basic_layer(self, num, inplane, start = False):\n",
    "        layer = []\n",
    "        if start:\n",
    "            layer.append(Basicblock(inplane, inplane))\n",
    "        else:\n",
    "            layer.append(Basicblock(inplane//2, inplane, stride = 2))\n",
    "        for i in range(num-1):\n",
    "            layer.append(Basicblock(inplane, inplane))\n",
    "        return nn.Sequential(*layer)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv0(inputs)\n",
    "        x = self.bn(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.pool2(x)\n",
    "        x = paddle.squeeze(x)\n",
    "        x = self.hidden5(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346b8bb2",
   "metadata": {},
   "source": [
    "训练仅有图片的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a6b8fd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch_id: 0, loss is: 1.409371018409729,acc is: 0.21875\n",
      "epoch: 0, batch_id: 50, loss is: 0.9275957345962524,acc is: 0.53125\n",
      "epoch: 0, batch_id: 100, loss is: 0.9793506860733032,acc is: 0.5\n",
      "[validation] accuracy: 0.606249988079071, loss: 0.9248269200325012\n",
      "epoch: 1, batch_id: 0, loss is: 1.0668646097183228,acc is: 0.53125\n",
      "epoch: 1, batch_id: 50, loss is: 1.0120793581008911,acc is: 0.53125\n",
      "epoch: 1, batch_id: 100, loss is: 1.0241944789886475,acc is: 0.5\n",
      "[validation] accuracy: 0.4333333373069763, loss: 1.2456109523773193\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train1(model):\n",
    "    model.train()\n",
    "    opt = paddle.optimizer.Adam(learning_rate=0.0002, parameters=model.parameters())\n",
    "    steps = 0\n",
    "    Iters, total_loss, total_acc = [], [], []\n",
    "    \n",
    "    for epoch in range(2):\n",
    "        # 块名称\n",
    "        for batch_id, data in enumerate(train_loader):\n",
    "            try:\n",
    "                steps += 1\n",
    "                image = data[0]  # 只使用图像数据\n",
    "                label = data[2]  # 使用标签数据\n",
    "\n",
    "                # 根据图像预测的数据\n",
    "                logits = model(image)  # 修改为仅基于图像输入\n",
    "\n",
    "                loss = paddle.nn.functional.cross_entropy(logits, label)\n",
    "                acc = paddle.metric.accuracy(logits, label)\n",
    "\n",
    "                if batch_id % 50 == 0:\n",
    "                    Iters.append(steps)\n",
    "                    total_loss.append(loss.numpy())\n",
    "                    total_acc.append(acc.numpy())\n",
    "                    print(\"epoch: {}, batch_id: {}, loss is: {}, acc is: {}\".format(epoch, batch_id, loss.numpy(), acc.numpy()))\n",
    "\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                opt.clear_grad()\n",
    "\n",
    "            except IndexError:\n",
    "                # print(e)\n",
    "                # print(f\"Train Warning: Encountered an indexing error at epoch {epoch}, batch_id {batch_id}. Skipping this batch.\")\n",
    "                continue\n",
    "\n",
    "        # Evaluate model after one epoch\n",
    "        model.eval()\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "\n",
    "        for batch_id, data in enumerate(test_loader):\n",
    "            try:\n",
    "                image = data[0]  # 只使用图像数据\n",
    "                label = data[2]  # 使用标签数据\n",
    "\n",
    "                logits = model(image)  # 修改为仅基于图像输入\n",
    "                loss = paddle.nn.functional.cross_entropy(logits, label)\n",
    "                acc = paddle.metric.accuracy(logits, label)\n",
    "\n",
    "                accuracies.append(acc.numpy())\n",
    "                losses.append(loss.numpy())\n",
    "\n",
    "            except IndexError:\n",
    "                print(f\"Val Warning: Encountered an indexing error at epoch {epoch}, batch_id {batch_id}. Skipping this batch.\")\n",
    "                continue\n",
    "\n",
    "        avg_acc, avg_loss = np.mean(accuracies), np.mean(losses)\n",
    "        print(\"[validation] accuracy: {}, loss: {}\".format(avg_acc, avg_loss))\n",
    "        model.train()\n",
    "\n",
    "\n",
    "\n",
    "num_blocks = [2, 2, 2, 2]\n",
    "\n",
    "image_model = ImageModel(num_blocks, bottlenet=False)\n",
    "train1(image_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfd13d4",
   "metadata": {},
   "source": [
    "### 生成仅有文本的模型\n",
    "\n",
    "这段代码定义了一个基于PaddlePaddle框架的文本分类模型（TextModel），它继承自paddle.nn.Layer。模型包含一个词嵌入层、一系列卷积层和池化层，以及全连接层用于特征提取与分类。在初始化时设置了模型参数，并在forward方法中定义了前向传播过程，将输入文本序列经过嵌入、一维卷积、ReLU激活、最大池化等操作后，通过两层全连接层映射到最终的类别输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "79c4c0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TextModel(paddle.nn.Layer):\n",
    "    def __init__(self, vocab_size, emb_dim=128, hid_dim=128, fc_hid_dim=96, class_dim=2, seq_len=150):\n",
    "        super(TextModel, self).__init__()\n",
    "        self.embedding = paddle.nn.Embedding(vocab_size, emb_dim, sparse=False)\n",
    "        self.hidden1 = paddle.nn.Conv2D(in_channels=1, out_channels=hid_dim, kernel_size=[3, emb_dim], padding=[1, 1])\n",
    "        self.relu1 = paddle.nn.ReLU()\n",
    "        self.hidden3 = paddle.nn.MaxPool2D(kernel_size=2, stride=2)\n",
    "        self.dict_dim = vocab[\"<pad>\"]\n",
    "        self.emb_dim = 128\n",
    "        self.hid_dim = 128\n",
    "        self.fc_hid_dim = 96\n",
    "        self.class_dim = 2\n",
    "        self.channels = 1\n",
    "        self.win_size = [3, self.hid_dim]\n",
    "        self.batch_size = 32\n",
    "        self.seq_len = 150\n",
    "        self.embedding = Embedding(self.dict_dim + 1, self.emb_dim, sparse=False)\n",
    "        self.hidden1 = paddle.nn.Conv2D(in_channels=1,                        #通道数\n",
    "                                            out_channels=self.hid_dim,        #卷积核个数\n",
    "                                            kernel_size=self.win_size,        #卷积核大小\n",
    "                                            padding=[1, 1]\n",
    "                                            )                         \n",
    "        self.relu1 = paddle.nn.ReLU()\n",
    "        self.hidden3 = paddle.nn.MaxPool2D(kernel_size=2,         #池化核大小\n",
    "                                            stride=2)             #池化步长2\n",
    "        self.hidden4 = paddle.nn.Linear(128*75, 512)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.conv0 = Conv2D(3, 64, 7, stride=2)\n",
    "        self.bn = BatchNorm2D(64)\n",
    "        self.pool1 = MaxPool2D(3, stride=2)\n",
    "        \n",
    "        # 文本部分初始化\n",
    "        self.embedding = paddle.nn.Embedding(vocab_size, emb_dim, sparse=False)\n",
    "        self.hidden1 = paddle.nn.Conv2D(in_channels=1, out_channels=hid_dim, kernel_size=[3, emb_dim], padding=[1, 1])\n",
    "        self.relu1 = paddle.nn.ReLU()\n",
    "        self.hidden3 = paddle.nn.MaxPool2D(kernel_size=2, stride=2)\n",
    "        \n",
    "        # 根据seq_len计算隐藏层输出维度以适配Linear层输入\n",
    "        linear_in_features = (seq_len // 4) * hid_dim\n",
    "        \n",
    "        self.hidden4 = paddle.nn.Linear(linear_in_features, fc_hid_dim)\n",
    "        self.out_layer = paddle.nn.Linear(fc_hid_dim, class_dim)\n",
    "\n",
    "    def forward(self, input_sent):  \n",
    "        batch_size = input_sent.shape[0]  # 动态获取批次大小\n",
    "\n",
    "        x = self.embedding(input_sent)\n",
    "        x = paddle.reshape(x, shape=[batch_size, 1, self.seq_len, self.emb_dim])   \n",
    "        x = self.hidden1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.hidden3(x)\n",
    "        x = paddle.reshape(x, shape=[-1, self.hidden4.weight.shape[1]])  # 使用全连接层权重的输入维度来动态调整reshape的尺寸\n",
    "        x = self.hidden4(x)\n",
    "        x = self.out_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d9c373",
   "metadata": {},
   "source": [
    "训练仅有文本的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b2192832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch_id: 0, loss is: 1.9350683689117432,acc is: 0.3125\n",
      "epoch: 0, batch_id: 50, loss is: 1.3322980403900146,acc is: 0.5\n",
      "epoch: 0, batch_id: 100, loss is: 1.0839498043060303,acc is: 0.5\n",
      "[validation] accuracy: 0.550000011920929, loss: 1.0302530527114868\n",
      "epoch: 1, batch_id: 0, loss is: 0.8008334040641785,acc is: 0.625\n",
      "epoch: 1, batch_id: 50, loss is: 0.8347412943840027,acc is: 0.625\n",
      "epoch: 1, batch_id: 100, loss is: 0.8118945956230164,acc is: 0.5625\n",
      "[validation] accuracy: 0.5541666746139526, loss: 1.098115086555481\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# def train(model, data_loader):\n",
    "#     for batch in data_loader:\n",
    "#         sent, labels, batch_size = batch[0], batch[2], len(batch[0])  # 假设sent是文本序列，labels是标签，这里获取当前批次的实际大小\n",
    "#         logits = model(sent, batch_size)  # 传递batch_size\n",
    "def train(model):\n",
    "    model.train()\n",
    "    opt = paddle.optimizer.Adam(learning_rate=0.0002, parameters=model.parameters())\n",
    "    steps = 0\n",
    "    Iters, total_loss, total_acc = [], [], []\n",
    "    for epoch in range(2):\n",
    "        # 块名称\n",
    "        for batch_id, data in enumerate(train_loader):\n",
    "            try:\n",
    "                steps += 1\n",
    "                image = data[0]\n",
    "                sent = data[1]\n",
    "                label = data[2]\n",
    "                # 根据图像和文本预测的数据\n",
    "                # logits = model(image,sent)\n",
    "                # logits = model(sent)\n",
    "                # sent, labels, batch_size = batch[0], batch[2], len(batch[0])  # 假设sent是文本序列，labels是标签，这里获取当前批次的实际大小\n",
    "                logits = model(sent)  # 传递batch_size\n",
    "                # print(logits)\n",
    "                # 损失的定义（交叉熵损失函数）\n",
    "                loss = paddle.nn.functional.cross_entropy(logits, label)\n",
    "                # print(\"loss\",loss)\n",
    "                acc = paddle.metric.accuracy(logits, label)\n",
    "                # print(\"acc\",acc)\n",
    "                if batch_id % 50 == 0:\n",
    "                    Iters.append(steps)\n",
    "                    # 原先代码\n",
    "                    # total_loss.append(loss.numpy()[0])\n",
    "                    # print(loss.numpy())\n",
    "                    # print(loss.numpy()[0])\n",
    "                    # total_acc.append(acc.numpy()[0])\n",
    "                    total_loss.append(loss.numpy())\n",
    "                    total_acc.append(acc.numpy())\n",
    "                    print(\"epoch: {}, batch_id: {}, loss is: {},acc is: {}\".format(epoch, batch_id, loss.numpy(),acc.numpy()))\n",
    "                \n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                opt.clear_grad()\n",
    "            \n",
    "            except IndexError:\n",
    "                print(f\"Warning: Encountered an indexing error at epoch {epoch}, batch_id {batch_id}. Skipping this batch.\")\n",
    "                continue\n",
    "\n",
    "        # evaluate model after one epoch\n",
    "        model.eval()\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        for batch_id, data in enumerate(test_loader):\n",
    "            try:\n",
    "                # sent = data[0]\n",
    "                # #print(type(sent))\n",
    "                # label = data[1]\n",
    "                # logits = model(sent)\n",
    "                image=data[0]\n",
    "                sent = data[1]\n",
    "                label = data[2]\n",
    "                logits = model(sent)\n",
    "                loss = paddle.nn.functional.cross_entropy(logits, label)\n",
    "                acc = paddle.metric.accuracy(logits, label)\n",
    "                accuracies.append(acc.numpy())\n",
    "                losses.append(loss.numpy())\n",
    "            except IndexError:\n",
    "                print(f\"Warning: Encountered an indexing error at epoch {epoch}, batch_id {batch_id}. Skipping this batch.\")\n",
    "                continue\n",
    "            \n",
    "        avg_acc, avg_loss = np.mean(accuracies), np.mean(losses)\n",
    "        print(\"[validation] accuracy: {}, loss: {}\".format(avg_acc, avg_loss))\n",
    "        model.train()\n",
    "\n",
    "  \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "text_model = TextModel(len(vocab))\n",
    "train(text_model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6216ab",
   "metadata": {},
   "source": [
    "### 下面是无关代码，仅记录留念"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f628e8",
   "metadata": {},
   "source": [
    "预测测试集标签试错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bee657c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train(model)\n",
    "# # path2label1 = {}\n",
    "# class_dict = {str(i): label for i, label in enumerate(['negative', 'neutral', 'positive'])}\n",
    "# with open('dataset/test1.txt', 'w') as f:\n",
    "#     # for batch_id, data in enumerate(train_loader):\n",
    "#     #     # 假设data是一个元组，其中包含了(image_path, input_sent, true_label)\n",
    "#     #     image_path, input_sent, true_label = data[0], data[1], data[2]\n",
    "#     #     # print(data[0])\n",
    "#     #     # print(data[1])\n",
    "#     #     # print(data[2])\n",
    "#     #     pred_logits = model(image_path, input_sent)\n",
    "#     #     print(pred_logits)\n",
    "#     #     pred_label_index = paddle.argmax(pred_logits).numpy().item()\n",
    "#     #     print(pred_label_index)\n",
    "\n",
    "#     for batch_id, data in enumerate(last_loader):\n",
    "\n",
    "#         # 假设data是一个元组，其中包含了(image_path, input_sent, true_label)\n",
    "#         image_path, input_sent, true_label = data[0], data[1], data[2]\n",
    "#         # print(data[0])\n",
    "#         # print(data[1])\n",
    "#         # print(data[2])\n",
    "\n",
    "#         # # 使用模型进行预测，这里假设model返回的是类别的索引\n",
    "#         # pred_logits = model(image_path, input_sent)\n",
    "\n",
    "#         # #  # 将 logits 转换为概率分布\n",
    "#         # # probs = F.softmax(pred_logits, axis=-1)  # 对于多分类问题，axis=-1 表示对最后一维进行 softmax 操作\n",
    "\n",
    "#         # # print(pred_logits)\n",
    "#         # pred_label_index = paddle.argmax(pred_logits).numpy().item()\n",
    "#         # print(pred_label_index)\n",
    "\n",
    "#         # 在模型前向传播计算之后获取 logits\n",
    "#         logits = model(image_path, input_sent)\n",
    "\n",
    "#         # 将 logits 转换为概率分布\n",
    "#         probs = F.softmax(logits, axis=-1)  # 对于多分类问题，axis=-1 表示对最后一维进行 softmax 操作\n",
    "\n",
    "#         # 获取预测类别（标签）的索引，这里假设返回的是一个批次的数据，每个样本都有一个预测类别\n",
    "#         pred_label_index = paddle.argmax(probs, axis=1)\n",
    "#         real_label= int(true_label[0].numpy()[0])\n",
    "#         print(real_label)\n",
    "#         pred_label = class_dict[str(pred_label_index[i].numpy())]\n",
    "#         f.write(f\"{real_label},{pred_label}\\n\")\n",
    "\n",
    "#         # # 如果需要将索引映射回实际类别名称，可以使用预定义的类别字典\n",
    "#         # class_labels = [class_dict[label_id] for label_id in predicted_labels.numpy()]\n",
    "        \n",
    "#         # for i in range(pred_label_index.shape[0]):  # 遍历批次中的样本\n",
    " \n",
    "#         #     # 将索引转换为标签\n",
    "#         #     if pred_label_index[i] == 0:\n",
    "#         #         pred_label = \"negative\"\n",
    "#         #     elif pred_label_index[i] == 1:\n",
    "#         #         pred_label = \"neutral\"\n",
    "#         #     elif pred_label_index[i] == 2:\n",
    "#         #         pred_label = \"positive\"\n",
    "#         #     else :\n",
    "#         #         pred_label = \"?\"\n",
    "\n",
    "#             # 更新字典并将预测结果写入文件\n",
    "#             # path2label1[image_path] = pred_label\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be608ef",
   "metadata": {},
   "source": [
    "模型试错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e576b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import paddle\n",
    "# import paddle.nn as nn\n",
    "# from paddle.nn import Conv2D, MaxPool2D, AdaptiveAvgPool2D, Linear, ReLU, BatchNorm2D\n",
    "# import paddle.nn.functional as F\n",
    "\n",
    "# class Basicblock(paddle.nn.Layer):\n",
    "#     def __init__(self, in_channel, out_channel, stride = 1):\n",
    "#         super(Basicblock, self).__init__()\n",
    "#         self.stride = stride\n",
    "#         self.conv0 = Conv2D(in_channel, out_channel, 3, stride = stride, padding = 1)\n",
    "#         self.conv1 = Conv2D(out_channel, out_channel, 3, stride=1, padding = 1)\n",
    "#         self.conv2 = Conv2D(in_channel, out_channel, 1, stride = stride)\n",
    "#         self.bn0 = BatchNorm2D(out_channel)\n",
    "#         self.bn1 = BatchNorm2D(out_channel)\n",
    "#         self.bn2 = BatchNorm2D(out_channel)\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         y = inputs\n",
    "#         x = self.conv0(inputs)\n",
    "#         x = self.bn0(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         if self.stride == 2:\n",
    "#             y = self.conv2(y)\n",
    "#             y = self.bn2(y)\n",
    "#         z = F.relu(x+y)\n",
    "#         return z\n",
    "\n",
    "# class Bottleneckblock(paddle.nn.Layer):\n",
    "#     def __init__(self, inplane, in_channel, out_channel, stride = 1, start = False):\n",
    "#         super(Bottleneckblock, self).__init__()\n",
    "#         self.stride = stride\n",
    "#         self.start = start\n",
    "#         self.conv0 = Conv2D(in_channel, inplane, 1, stride = stride)\n",
    "#         self.conv1 = Conv2D(inplane, inplane, 3, stride=1, padding=1)\n",
    "#         self.conv2 = Conv2D(inplane, out_channel, 1, stride=1)\n",
    "#         self.conv3 = Conv2D(in_channel, out_channel, 1, stride = stride)\n",
    "#         self.bn0 = BatchNorm2D(inplane)\n",
    "#         self.bn1 = BatchNorm2D(inplane)\n",
    "#         self.bn2 = BatchNorm2D(out_channel)\n",
    "#         self.bn3 = BatchNorm2D(out_channel)\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         y = inputs\n",
    "#         x = self.conv0(inputs)\n",
    "#         x = self.bn0(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.bn2(x)\n",
    "#         if self.start:\n",
    "#             y = self.conv3(y)\n",
    "#             y = self.bn3(y)\n",
    "#         z = F.relu(x+y)\n",
    "#         return z\n",
    "\n",
    "# # class Resnet(paddle.nn.Layer):\n",
    "# #     def __init__(self, ):\n",
    "# #         super(Resnet, self).__init__()\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "# # def resnet18():\n",
    "# #     return Resnet()\n",
    "\n",
    "# #定义卷积网络\n",
    "# class CNN(paddle.nn.Layer):\n",
    "#     def __init__(self,num, bottlenet):\n",
    "#         super(CNN,self).__init__()\n",
    "#         self.dict_dim = vocab[\"<pad>\"]\n",
    "#         self.emb_dim = 128\n",
    "#         self.hid_dim = 128\n",
    "#         self.fc_hid_dim = 96\n",
    "#         self.class_dim = 2\n",
    "#         self.channels = 1\n",
    "#         self.win_size = [3, self.hid_dim]\n",
    "#         self.batch_size = 32\n",
    "#         self.seq_len = 150\n",
    "#         self.embedding = Embedding(self.dict_dim + 1, self.emb_dim, sparse=False)\n",
    "#         self.hidden1 = paddle.nn.Conv2D(in_channels=1,                        #通道数\n",
    "#                                             out_channels=self.hid_dim,        #卷积核个数\n",
    "#                                             kernel_size=self.win_size,        #卷积核大小\n",
    "#                                             padding=[1, 1]\n",
    "#                                             )                         \n",
    "#         self.relu1 = paddle.nn.ReLU()\n",
    "#         self.hidden3 = paddle.nn.MaxPool2D(kernel_size=2,         #池化核大小\n",
    "#                                             stride=2)             #池化步长2\n",
    "#         self.hidden4 = paddle.nn.Linear(128*75, 512)\n",
    "\n",
    "#         self.conv0 = Conv2D(3, 64, 7, stride=2)\n",
    "#         self.bn = BatchNorm2D(64)\n",
    "#         self.pool1 = MaxPool2D(3, stride=2)\n",
    "#         if bottlenet:\n",
    "#             self.layer0 = self.add_bottleneck_layer(num[0], 64, start = True)\n",
    "#             self.layer1 = self.add_bottleneck_layer(num[1], 128)\n",
    "#             self.layer2 = self.add_bottleneck_layer(num[2], 256)\n",
    "#             self.layer3 = self.add_bottleneck_layer(num[3], 512)\n",
    "#         else:\n",
    "#             self.layer0 = self.add_basic_layer(num[0], 64, start = True)\n",
    "#             self.layer1 = self.add_basic_layer(num[1], 128)\n",
    "#             self.layer2 = self.add_basic_layer(num[2], 256)\n",
    "#             self.layer3 = self.add_basic_layer(num[3], 512)\n",
    "#         self.pool2 = AdaptiveAvgPool2D(output_size = (1, 1))\n",
    "#         self.hidden5 = paddle.nn.Linear(512, 3)\n",
    "\n",
    "#     def add_bottleneck_layer(self, num, inplane, start = False):\n",
    "#         layer = []\n",
    "#         if start:\n",
    "#             layer.append(Bottleneckblock(inplane, inplane, inplane*4, start = True))\n",
    "#         else:\n",
    "#             layer.append(Bottleneckblock(inplane, inplane*2, inplane*4, stride = 2, start = True))\n",
    "#         for i in range(num-1):\n",
    "#             layer.append(Bottleneckblock(inplane, inplane*4, inplane*4))\n",
    "#         return nn.Sequential(*layer)\n",
    "\n",
    "#     #def forward(self, inputs):\n",
    "       \n",
    "\n",
    "    # def add_basic_layer(self, num, inplane, start = False):\n",
    "    #     layer = []\n",
    "    #     if start:\n",
    "    #         layer.append(Basicblock(inplane, inplane))\n",
    "    #     else:\n",
    "    #         layer.append(Basicblock(inplane//2, inplane, stride = 2))\n",
    "    #     for i in range(num-1):\n",
    "    #         layer.append(Basicblock(inplane, inplane))\n",
    "    #     return nn.Sequential(*layer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     #网络的前向计算过程\n",
    "#     def forward(self,inputs,input):\n",
    "        \n",
    "#         #print('输入维度：', input.shape)\n",
    "#         x = self.embedding(input)\n",
    "#         x = paddle.reshape(x, [32, 1, 150, 128])   \n",
    "#         x = self.hidden1(x)\n",
    "#         x = self.relu1(x)\n",
    "#         #print('第一层卷积输出维度：', x.shape)\n",
    "#         x = self.hidden3(x)\n",
    "#         #print('池化后输出维度：', x.shape)\n",
    "#         #在输入全连接层时，需将特征图拉平会自动将数据拉平.\n",
    "\n",
    "#         x = paddle.reshape(x, shape=[self.batch_size, -1])\n",
    "#         out1 = self.hidden4(x)\n",
    "\n",
    "\n",
    "\n",
    "#         x = self.conv0(inputs)\n",
    "#         x = self.bn(x)\n",
    "#         x = self.pool1(x)\n",
    "#         x = self.layer0(x)\n",
    "#         x = self.layer1(x)\n",
    "#         x = self.layer2(x)\n",
    "#         x = self.layer3(x)        \n",
    "#         x = self.pool2(x)\n",
    "#         x = paddle.squeeze(x)\n",
    "#         out=out1+x\n",
    "#         out=self.hidden5(out)\n",
    "\n",
    "#         return out\n",
    "\n",
    "\n",
    "\n",
    "# # class TextModel(paddle.nn.Layer):\n",
    "# #     def __init__(self, vocab_size, emb_dim=128, hid_dim=128, fc_hid_dim=96, class_dim=2, seq_len=150):\n",
    "# #         super(TextModel, self).__init__()\n",
    "# #         # 文本部分的初始化与原来CNN类中的相关结构相同\n",
    "# #         self.dict_dim = vocab[\"<pad>\"]\n",
    "# #         self.emb_dim = 128\n",
    "# #         self.hid_dim = 128\n",
    "# #         self.fc_hid_dim = 96\n",
    "# #         self.class_dim = 2\n",
    "# #         self.channels = 1\n",
    "# #         self.win_size = [3, self.hid_dim]\n",
    "# #         self.batch_size = 32\n",
    "# #         self.seq_len = 150\n",
    "# #         self.embedding = Embedding(self.dict_dim + 1, self.emb_dim, sparse=False)\n",
    "#     #     self.hidden1 = paddle.nn.Conv2D(in_channels=1,                        #通道数\n",
    "#     #                                         out_channels=self.hid_dim,        #卷积核个数\n",
    "#     #                                         kernel_size=self.win_size,        #卷积核大小\n",
    "#     #                                         padding=[1, 1]\n",
    "#     #                                         )                         \n",
    "#     #     self.relu1 = paddle.nn.ReLU()\n",
    "#     #     self.hidden3 = paddle.nn.MaxPool2D(kernel_size=2,         #池化核大小\n",
    "#     #                                         stride=2)             #池化步长2\n",
    "#     #     self.hidden4 = paddle.nn.Linear(128*75, 512)\n",
    "\n",
    "#     #     self.conv0 = Conv2D(3, 64, 7, stride=2)\n",
    "#     #     self.bn = BatchNorm2D(64)\n",
    "#     #     self.pool1 = MaxPool2D(3, stride=2)\n",
    "#     #     self.pool2 = AdaptiveAvgPool2D(output_size = (1, 1))\n",
    "#     #     self.hidden5 = paddle.nn.Linear(512, 3)\n",
    "#     #     self.embedding = Embedding(vocab_size, emb_dim, sparse=False)\n",
    "#     #     self.hidden1 = paddle.nn.Conv2D(in_channels=1, out_channels=hid_dim, kernel_size=self.win_size, padding=[1, 1])\n",
    "#     #     self.relu1 = paddle.nn.ReLU()\n",
    "#     #     self.hidden3 = paddle.nn.MaxPool2D(kernel_size=2, stride=2)\n",
    "#     #     self.hidden4 = paddle.nn.Linear(128 * (seq_len // 4), fc_hid_dim)\n",
    "#     #     self.out_layer = paddle.nn.Linear(fc_hid_dim, class_dim)\n",
    "\n",
    "#     # def forward(self, input_sent):\n",
    "#         # x = self.embedding(input_sent)\n",
    "#         # x = paddle.reshape(x, [self.batch_size, 1, self.seq_len, self.emb_dim])   \n",
    "#         # x = self.hidden1(x)\n",
    "#         # x = self.relu1(x)\n",
    "#         # x = self.hidden3(x)\n",
    "#         # x = paddle.reshape(x, shape=[-1, self.fc_hid_dim])\n",
    "#         # x = self.hidden4(x)\n",
    "#         # x = self.out_layer(x)\n",
    "#         # return x\n",
    "# class TextModel(paddle.nn.Layer):\n",
    "#     def __init__(self, vocab_size, emb_dim=128, hid_dim=128, fc_hid_dim=96, class_dim=2, seq_len=150):\n",
    "#         super(TextModel, self).__init__()\n",
    "        \n",
    "#         # 文本部分初始化\n",
    "#         self.embedding = paddle.nn.Embedding(vocab_size, emb_dim, sparse=False)\n",
    "#         self.hidden1 = paddle.nn.Conv2D(in_channels=1, out_channels=hid_dim, kernel_size=[3, emb_dim], padding=[1, 1])\n",
    "#         self.relu1 = paddle.nn.ReLU()\n",
    "#         self.hidden3 = paddle.nn.MaxPool2D(kernel_size=2, stride=2)\n",
    "        \n",
    "#         # 根据seq_len计算隐藏层输出维度以适配Linear层输入\n",
    "#         linear_in_features = hid_dim * (seq_len // 4)\n",
    "        \n",
    "#         self.hidden4 = paddle.nn.Linear(linear_in_features, fc_hid_dim)\n",
    "#         self.out_layer = paddle.nn.Linear(fc_hid_dim, class_dim)\n",
    "\n",
    "#     def forward(self, input_sent):\n",
    "#         x = self.embedding(input_sent)\n",
    "#         x = paddle.reshape(x, [self.batch_size, 1, self.seq_len, self.embedding.embedding_dim])   \n",
    "#         x = self.hidden1(x)\n",
    "#         x = self.relu1(x)\n",
    "#         x = self.hidden3(x)\n",
    "#         x = paddle.reshape(x, shape=[-1, self.fc_hid_dim])\n",
    "#         x = self.hidden4(x)\n",
    "#         x = self.out_layer(x)\n",
    "#         return x\n",
    "\n",
    "# class ImageModel(paddle.nn.Layer):\n",
    "#     def __init__(self, num_blocks, bottlenet=True, class_dim=2):  # 添加class_dim参数\n",
    "#         super(ImageModel, self).__init__()\n",
    "        \n",
    "#         self.conv0 = Conv2D(3, 64, 7, stride=2)\n",
    "#         self.bn = BatchNorm2D(64)\n",
    "#         self.pool1 = MaxPool2D(3, stride=2)\n",
    "\n",
    "#         if bottlenet:\n",
    "#             self.layer0 = self.add_bottleneck_layer(num_blocks[0], 64, start=True)\n",
    "#             self.layer1 = self.add_bottleneck_layer(num_blocks[1], 128)\n",
    "#             self.layer2 = self.add_bottleneck_layer(num_blocks[2], 256)\n",
    "#             self.layer3 = self.add_bottleneck_layer(num_blocks[3], 512)\n",
    "#         else:\n",
    "#             self.layer0 = self.add_basic_layer(num[0], 64, start = True)\n",
    "#             self.layer1 = self.add_basic_layer(num[1], 128)\n",
    "#             self.layer2 = self.add_basic_layer(num[2], 256)\n",
    "#             self.layer3 = self.add_basic_layer(num[3], 512)\n",
    "\n",
    "#         self.pool2 = AdaptiveAvgPool2D(output_size=(1, 1))\n",
    "#         self.hidden5 = paddle.nn.Linear(512, class_dim)  # 修改为正确的输入维度和类别数量\n",
    "\n",
    "#     def add_bottleneck_layer(self, num, inplane, start=False):\n",
    "#         layer = []\n",
    "#         if start:\n",
    "#             layer.append(BottleneckBlock(inplane, inplane, inplane*4, start=start))\n",
    "#         else:\n",
    "#             layer.append(BottleneckBlock(inplane, inplane*2, inplane*4, stride=2, start=start))\n",
    "#         for _ in range(num - 1):\n",
    "#             layer.append(BottleneckBlock(inplane, inplane*4, inplane*4))\n",
    "#         return nn.Sequential(*layer)\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         x = self.conv0(inputs)\n",
    "#         x = self.bn(x)\n",
    "#         x = self.pool1(x)\n",
    "#         x = self.layer0(x)\n",
    "#         x = self.layer1(x)\n",
    "#         x = self.layer2(x)\n",
    "#         x = self.layer3(x)\n",
    "#         x = self.pool2(x)\n",
    "#         x = paddle.squeeze(x)\n",
    "#         x = self.hidden5(x)\n",
    "#         return x\n",
    "\n",
    "# # class ImageModel(paddle.nn.Layer):\n",
    "# #     def __init__(self, num, bottlenet=True):\n",
    "# #         super(ImageModel, self).__init__()\n",
    "# #         # 图像部分的初始化与原来CNN类中的相关结构相同\n",
    "# #         self.dict_dim = vocab[\"<pad>\"]\n",
    "# #         self.emb_dim = 128\n",
    "# #         self.hid_dim = 128\n",
    "# #         self.fc_hid_dim = 96\n",
    "# #         self.class_dim = 2\n",
    "# #         self.channels = 1\n",
    "# #         self.win_size = [3, self.hid_dim]\n",
    "# #         self.batch_size = 32\n",
    "# #         self.seq_len = 150\n",
    "# #         self.embedding = Embedding(self.dict_dim + 1, self.emb_dim, sparse=False)\n",
    "# #         self.hidden1 = paddle.nn.Conv2D(in_channels=1,                        #通道数\n",
    "# #                                             out_channels=self.hid_dim,        #卷积核个数\n",
    "# #                                             kernel_size=self.win_size,        #卷积核大小\n",
    "# #                                             padding=[1, 1]\n",
    "# #                                             )                         \n",
    "# #         self.relu1 = paddle.nn.ReLU()\n",
    "# #         self.hidden3 = paddle.nn.MaxPool2D(kernel_size=2,         #池化核大小\n",
    "# #                                             stride=2)             #池化步长2\n",
    "# #         self.hidden4 = paddle.nn.Linear(128*75, 512)\n",
    "\n",
    "# #         self.conv0 = Conv2D(3, 64, 7, stride=2)\n",
    "# #         self.bn = BatchNorm2D(64)\n",
    "# #         self.pool1 = MaxPool2D(3, stride=2)\n",
    "# #         if bottlenet:\n",
    "# #             self.layer0 = self.add_bottleneck_layer(num[0], 64, start = True)\n",
    "# #             self.layer1 = self.add_bottleneck_layer(num[1], 128)\n",
    "# #             self.layer2 = self.add_bottleneck_layer(num[2], 256)\n",
    "# #             self.layer3 = self.add_bottleneck_layer(num[3], 512)\n",
    "# #         else:\n",
    "#     #         self.layer0 = self.add_basic_layer(num[0], 64, start = True)\n",
    "#     #         self.layer1 = self.add_basic_layer(num[1], 128)\n",
    "#     #         self.layer2 = self.add_basic_layer(num[2], 256)\n",
    "#     #         self.layer3 = self.add_basic_layer(num[3], 512)\n",
    "#     #     self.pool2 = AdaptiveAvgPool2D(output_size = (1, 1))\n",
    "\n",
    "#     #     self.hidden5 = paddle.nn.Linear(512, self.class_dim)  # 假设这里的class_dim与文本分类任务一致\n",
    "\n",
    "#     # def add_bottleneck_layer(self, num, inplane, start = False):\n",
    "#     #     layer = []\n",
    "#     #     if start:\n",
    "#     #         layer.append(Bottleneckblock(inplane, inplane, inplane*4, start = True))\n",
    "#     #     else:\n",
    "#     #         layer.append(Bottleneckblock(inplane, inplane*2, inplane*4, stride = 2, start = True))\n",
    "#     #     for i in range(num-1):\n",
    "#     #         layer.append(Bottleneckblock(inplane, inplane*4, inplane*4))\n",
    "#     #     return nn.Sequential(*layer)\n",
    "\n",
    "#     # def forward(self, inputs):\n",
    "#     #     x = self.conv0(inputs)\n",
    "#     #     x = self.bn(x)\n",
    "#     #     x = self.pool1(x)\n",
    "#     #     x = self.layer0(x)\n",
    "#     #     x = self.layer1(x)\n",
    "#     #     x = self.layer2(x)\n",
    "#     #     x = self.layer3(x)\n",
    "#     #     x = self.pool2(x)\n",
    "#     #     x = paddle.squeeze(x)\n",
    "#     #     x = self.hidden5(x)\n",
    "#     #     return x\n",
    "\n",
    "# # # 然后分别训练这两个模型，并在验证集上评估它们的表现\n",
    "# # def train_and_evaluate_text_only(text_model, text_loader):\n",
    "# #     # 训练并评估仅使用文本的模型\n",
    "# #     pass\n",
    "\n",
    "# # def train_and_evaluate_image_only(image_model, image_loader):\n",
    "# #     # 训练并评估仅使用图像的模型\n",
    "# #     pass\n",
    "\n",
    "# # text_model = TextModel(len(vocab))\n",
    "# # # train_and_evaluate_text_only(text_, text_loader)\n",
    "# # num_blocks = [2,2,2,2]  # 或者从配置文件、命令行参数等地方获取\n",
    "\n",
    "# # image_model = ImageModel(num_blocks)\n",
    "# # # train_and_evaluate_image_only(image_model, image_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
